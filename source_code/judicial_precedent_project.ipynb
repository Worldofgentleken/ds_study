{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71347dbf",
   "metadata": {},
   "source": [
    "# KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e80449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reals\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0af9125e154c64b6c8224bd3dbbed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 판례 데이터와 벡터화\u001b[39;00m\n\u001b[0;32m     22\u001b[0m case_texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourt_detail\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 23\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([get_embedding(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(case_texts)])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 새로운 상황 텍스트와 벡터화\u001b[39;00m\n\u001b[0;32m     26\u001b[0m new_case_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m새로운 상황 텍스트\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 판례 데이터와 벡터화\u001b[39;00m\n\u001b[0;32m     22\u001b[0m case_texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourt_detail\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 23\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([\u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(case_texts)])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 새로운 상황 텍스트와 벡터화\u001b[39;00m\n\u001b[0;32m     26\u001b[0m new_case_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m새로운 상황 텍스트\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(text):\n\u001b[0;32m     13\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1137\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1137\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1149\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1150\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:690\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    679\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    680\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    681\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m         output_attentions,\n\u001b[0;32m    688\u001b[0m     )\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 690\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:580\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    570\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    577\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    579\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:510\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    502\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 510\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    520\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# mask in case tgt_len == 1.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 435\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    445\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# KoBERT 모델 및 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertModel.from_pretrained('monologg/kobert')\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "file_path = 'data/'\n",
    "\n",
    "df = pd.read_excel(f'{file_path}law_recomended_0712.xlsx')\n",
    "\n",
    "# 판례 데이터와 벡터화\n",
    "case_texts = df['court_detail'].tolist()\n",
    "embeddings = np.vstack([get_embedding(text) for text in tqdm(case_texts)])\n",
    "\n",
    "# 새로운 상황 텍스트와 벡터화\n",
    "new_case_text = \"새로운 상황 텍스트\"\n",
    "new_embedding = get_embedding(new_case_text)\n",
    "\n",
    "# 유사도 계산\n",
    "similarities = cosine_similarity(new_embedding, embeddings)\n",
    "\n",
    "# 상위 5개 판례 선택\n",
    "top_5_indices = similarities[0].argsort()[-5:][::-1]\n",
    "top_5_cases = [case_texts[i] for i in top_5_indices]\n",
    "print(top_5_cases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b2142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09221720",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\reals\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\reals\\AppData\\Local\\Temp\\ipykernel_23280\\1500348237.py\", line 1, in <cell line: 1>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\", line 77, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\reals\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\reals\\AppData\\Local\\Temp\\ipykernel_23280\\1500348237.py\", line 1, in <cell line: 1>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\", line 77, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\reals\\anaconda3\\lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/law_recomended.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/law_recomended.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/law_recomended.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('data/law_recomended.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e64d03a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: 'pip#sklearn': Expected end or semicolon (after name and no valid version specifier)\n",
      "    pip#sklearn\n",
      "       ^\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip #sklearn huggingface_hub transformers datasets langchain konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b195e7c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\reals\\anaconda3\\lib\\site-packages (24.1.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.1.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 1.8/1.8 MB 13.0 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\reals\\anaconda3\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f88cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_text_in_batches(df['prepro_detail'].tolist())\n",
    "\n",
    "# KMeans 군집화\n",
    "n_clusters = 7  # 원하는 군집 수 설정\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8bf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38946a96",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reals\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embed_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#df['prepro_detail'] = df['detail_mean'].apply(prepro_text)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprepro_detail\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetail_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(prepro_text)\n\u001b[1;32m---> 57\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_text\u001b[49m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprepro_detail\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# KMeans 군집화\u001b[39;00m\n\u001b[0;32m     60\u001b[0m n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m  \u001b[38;5;66;03m# 원하는 군집 수 설정\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embed_text' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import KMeans\n",
    "from konlpy.tag import Komoran\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 텍스트 전처리\n",
    "def prepro_text(text) :\n",
    "    #text = re.sub(r'\\w+', ' ', text)\n",
    "    tokens = komoran.morphs(text)\n",
    "    # 토큰을 공백으로 연결\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# 텍스트 임베딩 함수 (배치 처리)\n",
    "def embed_text_in_batches(text_list, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch_texts = text_list[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings)\n",
    "\n",
    "# colab 사용 시 /content/ 내 작업공간 사용 시 data/\n",
    "file_path = 'data/'\n",
    "model_name = \"beomi/KcELECTRA-base-v2022\"\n",
    "\n",
    "# 데이터, 모델, 토크나이저 로드\n",
    "f = open(f'{file_path}hugging_face_token_key.txt', 'r')\n",
    "my_key = f.readline()\n",
    "\n",
    "#os.environ['HUGGINGFACEHUB_API_TOKEN'] = my_key\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "df = pd.read_excel(f'{file_path}law_recomended_0712.xlsx').dropna(axis=0)\n",
    "df = df[:20000]\n",
    "# 전처리된 column 가져오기\n",
    "df['detail_mean'] = df['court_detail'].str.extract(r'【이    유】(.*)', expand=False)\n",
    "\n",
    "# 실행\n",
    "komoran = Komoran()\n",
    "#df = df[:20]\n",
    "\n",
    "df['detail_mean'] = df['detail_mean'].str.replace('\\n', '')\n",
    "df['detail_mean'] = df['detail_mean'].fillna('')\n",
    "#df['prepro_detail'] = df['detail_mean'].apply(prepro_text)\n",
    "df['prepro_detail'] = df['detail_mean'].apply(prepro_text)\n",
    "\n",
    "embeddings = embed_text_in_batches(df['prepro_detail'].tolist())\n",
    "\n",
    "# KMeans 군집화\n",
    "n_clusters = 7  # 원하는 군집 수 설정\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33fabfe6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 68742807552 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(batch_embeddings)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(embeddings)\n\u001b[1;32m---> 13\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprepro_detail\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# KMeans 군집화\u001b[39;00m\n\u001b[0;32m     16\u001b[0m n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m  \u001b[38;5;66;03m# 원하는 군집 수 설정\u001b[39;00m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36membed_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad() :\n\u001b[1;32m---> 22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     23\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:899\u001b[0m, in \u001b[0;36mElectraModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    895\u001b[0m     encoder_extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    897\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_project\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    908\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_project(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:193\u001b[0m, in \u001b[0;36mElectraEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    190\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    196\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 68742807552 bytes."
     ]
    }
   ],
   "source": [
    "# 텍스트 임베딩 함수 (배치 처리)\n",
    "def embed_text_in_batches(text_list, batch_size=1):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch_texts = text_list[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings)\n",
    "\n",
    "embeddings = embed_text_in_batches(df['prepro_detail'].tolist())\n",
    "\n",
    "# KMeans 군집화\n",
    "n_clusters = 7  # 원하는 군집 수 설정\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bbe3639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델과 데이터를 파일 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 모델 및 데이터 저장\n",
    "#joblib.dump(komoran, f'{file_path}komoran.pkl')\n",
    "joblib.dump(tokenizer, f'{file_path}tokenizer.pkl')\n",
    "torch.save(model.state_dict(), f'{file_path}/kc_electra_model.pth')\n",
    "joblib.dump(embeddings, f'{file_path}embeddings.pkl')\n",
    "joblib.dump(kmeans, f'{file_path}kmeans_model.pkl')\n",
    "df.to_pickle(f'{file_path}data_with_preprocessed_text.pkl')\n",
    "\n",
    "print(\"모델과 데이터를 파일 저장 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c175fd",
   "metadata": {},
   "source": [
    "# KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6105c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccb082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 파일 불러오기\n",
    "file_path = 'data/law_recomended.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 전처리된 column 가져오기\n",
    "df_detail = df['court_detail']\n",
    "df_detail = df_detail.str.extract(r'【이    유】(.*)', expand=False)\n",
    "\n",
    "# KoBERT 모델 및 토크나이저 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertModel.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 텍스트 데이터를 KoBERT 입력 형식으로 변환\n",
    "def preprocess_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "    return inputs\n",
    "\n",
    "# 문서 임베딩 생성 함수\n",
    "def get_embedding(text):\n",
    "    inputs = preprocess_text(text)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "# 모든 문서에 대해 임베딩 생성\n",
    "embeddings = df_detail.apply(lambda x: get_embedding(x) if pd.notnull(x) else None)\n",
    "\n",
    "# 결과 확인\n",
    "print(embeddings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b5a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 데이터를 문자열에서 숫자 배열로 변환하는 함수\n",
    "def str_to_array(s):\n",
    "    return np.fromstring(s.strip('[]'), sep=' ')\n",
    "\n",
    "# 임베딩 데이터 불러오기\n",
    "loaded_embeddings_df = pd.read_excel('data/embeddings_data.xlsx')\n",
    "\n",
    "# 문자열 데이터를 숫자 배열로 변환\n",
    "loaded_embeddings = loaded_embeddings_df.iloc[:, 0].apply(lambda x: str_to_array(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 임베딩 데이터를 DataFrame으로 변환 및 결측치 처리\n",
    "embeddings_array = np.vstack(loaded_embeddings.dropna())\n",
    "\n",
    "# PCA 차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(embeddings_array)\n",
    "\n",
    "# 시각화\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1])\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.title('PCA of Document Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_court(input_sentence, top_n=5):\n",
    "    # 입력 문장을 임베딩 변환\n",
    "    query_embedding = get_embedding(input_sentence).reshape(1, -1)\n",
    "    \n",
    "    # 유사도 계산\n",
    "    similarities = cosine_similarity(query_embedding, embeddings_array)[0]\n",
    "    similar_docs_indices = np.argsort(similarities)[-top_n:][::-1]  # 상위 N개 문서 인덱스\n",
    "    \n",
    "    recommendations = []\n",
    "    for idx in similar_docs_indices:\n",
    "        if similarities[idx] == 0:\n",
    "            break\n",
    "        recommendations.append((df.iloc[idx]['event_Title'], \n",
    "                                df.iloc[idx]['event_num'], \n",
    "                                df.iloc[idx]['event_type'], \n",
    "                                df.iloc[idx]['jPrecedent_num'], \n",
    "                                df.iloc[idx]['court_name'], \n",
    "                                df.iloc[idx]['court_detail'], \n",
    "                                df.iloc[idx]['court_reference'], \n",
    "                                df.iloc[idx]['court_Decision'], \n",
    "                                similarities[idx]*100))\n",
    "    \n",
    "    if len(recommendations) == 0:\n",
    "        return \"유사한 판례를 찾을 수 없습니다.\"\n",
    "    else:\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "# 문자열 데이터를 숫자 배열로 변환\n",
    "loaded_embeddings = loaded_embeddings_df.iloc[:, 0].apply(lambda x: str_to_array(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 임베딩 데이터를 DataFrame으로 변환 및 결측치 처리\n",
    "embeddings_array = np.vstack(loaded_embeddings.dropna())\n",
    "\n",
    "# 원본 데이터 불러오기\n",
    "df = pd.read_excel('data/law_recomended.xlsx')\n",
    "\n",
    "# 판례 추천\n",
    "query = input(\"상황을 작성해 주세요 (자세히 작성 시 검색에 도움이 됩니다.) :\\n\")\n",
    "recommended_court = recommend_court(query)\n",
    "\n",
    "if isinstance(recommended_court, str):\n",
    "    print(recommended_court)\n",
    "else:\n",
    "    court_detail_OX = input(\"판례 및 판시 사항을 보시겠습니까? (Y/N): \\n\")\n",
    "    for event_Title, event_num, event_type, jPrecedent_num, court_name, court_detail, court_reference, court_Decision, similarity in recommended_court:\n",
    "        print('==================================================================================================================')\n",
    "        if (court_detail_OX == 'N') or (court_detail_OX == 'n'):\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\n판례 일련번호 : {jPrecedent_num} \\n법원 : {court_name} \\n\\n참조조문 \\n{court_reference}\\n\\n\\n\")\n",
    "        else:\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\n판례 일련번호 : {jPrecedent_num} \\n법원 : {court_name} \\n\\n판례 내용 :\\n{court_detail} \\n\\n참조조문 \\n{court_reference} \\n\\n판시사항 :\\n{court_Decision}\\n\\n\\n\")\n",
    "        print('==================================================================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037b8f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reals\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator KMeans from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상황을 작성해 주세요 (자세히 작성 시 검색에 도움이 됩니다.) :\n",
      "술 먹고 집으로 가던 중 치마 입은 모르는 여자분의 엉덩이를 만지고 지나갔나봐요 정말 실수로 그랬다고 합니다  그 여자분은 바로 신고했는데 경찰 말이 동네 고등학생이라고 했네요\n",
      "판례 및 판시 사항을 보시겠습니까? (Y/N): \n",
      "y\n",
      "==================================================================================================================\n",
      "사건 명 : 유족급여및장의비부지급처분취소 (유사도: 91.57%)\n",
      "사건 번호 : 2015구합74692 \n",
      "사건 종류 : 일반행정\n",
      "판례 일련번호 : 186061 \n",
      "법원 : 서울행정법원 \n",
      "\n",
      "판례 내용 :\n",
      "【원    고】 【피    고】 근로복지공단【변론종결】2016. 2. 25.【주    문】  1. 원고의 청구를 기각한다.  2. 소송비용은 원고가 부담한다.【청구취지】  피고가 2014. 12. 23. 원고에 대하여 한 유족급여 및 장의비 부지급 처분을 취소한다.【이    유】1. 처분의 경위  가. 원고의 배우자인 망 소외 1[1964. (생월일 생략)생, 이하 ‘망인’이라 한다]은 1992. 7. 1. 유리병을 제조하는 주식회사 △△(이하 ‘소외 회사’라 한다)에 입사하여 □□공장 생산팀 제병 C조 반장으로 근무하였다.  나. 망인은 2014. 7. 16. 22:00경 야간근무를 위해 소외 회사에 출근하였다가 22:30경부터 소외 회사로부터 지급받은 야식비를 회식 불참자에게 분배하여 줄 것인지 여부를 놓고 같은 조 직원인 소외 2[1972. (생월일 생략)생]와 말다툼을 시작하게 되었다.  다. 말다툼이 10분 정도 진행되었을 때 소외 2가 망인에게 “야식비를 회식 불참자에게 나누어주지 않으면 이는 엄연히 갈취나 마찬가지이다.”라는 취지의 발언을 하였고, 이에 격분한 망인이 주먹으로 소외 2의 얼굴을 때리면서 몸싸움이 시작되어 두 사람은 서로 엉겨붙은 채 바닥을 수차례 구르게 되었다.  라. 이를 발견한 동료 직원들이 두 사람을 떼어놓아 망인은 작업장으로, 소외 2는 제병공 휴게실로 이동하게 되었는데, 분을 참지 못한 망인이 몇 분 뒤 대걸레 막대기를 들고 휴게실로 찾아와 소외 2에게 휘둘러 소외 2가 이를 손으로 막았고, 두 사람이 다시 엉겨붙었다가 동료 직원들의 만류로 분리되는 과정에서 망인의 기력이 갑자기 떨어졌으며, 망인은 휴게실 밖으로 나가다 그대로 쓰러졌는데, 이때의 시각이 23:36경이었다.  마. 그 후 망인은 ◇◇◇의료원◇◇병원으로 이송되었으나, 2014. 7. 17. 00:33경 사망하였고, 부검 결과 망인의 사인은 ‘급성 심장사(비후성 심근병증이 의심되는 심비대 및 중등도의 심장동맥경화 등에 기인)’로 밝혀졌다.  바. 이에 원고는 피고에게 유족급여 및 장의비를 청구하였으나, 피고는 2014. 12. 23. 아래와 같은 사유로 부지급 결정(이하 ‘이 사건 처분’이라 한다)을 하였다.    - 망인과 소외 2 사이에 벌어진 1차 말다툼과 몸싸움은 야식비 사용방법에 대한 다툼으로 볼 수 있으나, 2차 몸싸움(망인이 대걸레 막대기를 들고 휴게실로 찾아온 이후의 싸움을 가리킨다)은 회사 업무와 관련이 없는 개인적인 감정변화와 스트레스 때문에 빚어진 것으로서 그로 인하여 발생한 재해에 대하여는 업무와의 상당인과관계를 인정할 수 없다.  사. 원고는 이 사건 처분에 불복하여 피고에게 심사 청구를 하였으나, 2015. 3. 19. 기각 결정을 받았고, 산업재해보상보험 재심사위원회에 재심사 청구를 하였으나, 2015. 6. 25. 기각 재결을 받았다.[인정근거] 다툼 없는 사실, 갑 제1호증, 갑 제2호증의 1, 2, 갑 제3호증의 1, 2, 갑 제4호증의 1, 2, 갑 제5호증, 갑 제6호증의 1 내지 3, 갑 제7호증의 1, 2, 을 제1, 2호증의 각 기재, 증인 소외 2의 증언, 변론 전체의 취지2. 이 사건 처분의 적법 여부  가. 원고의 주장    망인과 소외 2가 싸우게 된 원인은 야식비 사용방법에 관한 의견차이에 있으므로, 이를 업무와 무관하다고 볼 수 없는 점, 피고는 망인과 소외 2 사이의 싸움을 1, 2차로 구분하여 평가하였으나, 두 행위가 시간적·장소적으로 명확히 구분된 것으로 볼 수 없는 점, 소외 2가 먼저 모욕적인 언사를 사용하여 망인을 도발한 점, 2차 싸움은 불과 3분 정도의 시간 동안 벌어진 것으로서 1차 싸움에 비해 망인의 사망에 대한 기여도가 크다고 단정할 수 없는 점 등의 사정을 종합할 때, 망인의 사망과 업무 사이에 상당인과관계가 인정되어야 한다.  나. 관계 법령    별지 관계 법령의 기재와 같다.  다. 판  단    1) 관련 법리산업재해보상보험법 제5조 제1호에서 말하는 ‘업무상의 재해’라 함은 근로자가 업무를 수행하던 중 그 업무에 기인하여 발생한 근로자의 부상·질병·장해 또는 사망을 뜻하는 것이므로 업무와 재해발생 사이에 인과관계가 있어야 하고 이는 주장하는 측에서 입증하여야 한다(대법원 1998. 5. 22. 선고 98두4740 판결 등 참조).      한편, 근로자가 타인의 폭력에 의하여 재해를 입은 경우라고 하더라도 그것이 직장안의 인간관계 또는 직무에 내재하거나 통상 수반하는 위험이 현실화되어 발생한 것으로서 업무와 사이에 상당인과관계가 있으면 업무상 재해로 인정하여야 할 것이지만, 가해자의 폭력행위가 피해자와의 사적인 관계에서 기인하였다거나 피해자가 직무의 한도를 넘어 상대방을 자극하거나 도발함으로써 발생한 경우에는 업무기인성을 인정할 수 없어 업무상 재해로 볼 수 없다고 할 것이다(대법원 1995. 1. 24. 선고 94누8587 판결 등 참조).    2) 이 사건의 경우      갑 제6호증의 2, 5의 각 기재, 이 법원의 ☆☆☆☆☆장에 대한 진료기록감정촉탁결과에 변론 전체의 취지를 종합하면, 망인은 비대성 심근병증 등의 기존 질환이 있는 상태에서 소외 2와 몸싸움을 동반한 다툼을 벌이는 과정에서 발생한 급격한 감정변화, 스트레스 등의 자극으로 인하여 기존의 심장질환이 자연경과 이상으로 빠르게 악화되어 사망에 이르게 된 사실이 인정되고, 위 싸움이 야식비 사용방법에 관한 조원과의 의견교환 과정에서 촉발되었음은 원고 주장과 같다.      그러나 망인의 사망 원인이 된 위 싸움(피고는 동료 직원들의 만류로 망인과 소외 2가 서로 다른 공간에 3~4분 정도 분리되어 있었다는 이유로 이를 1, 2차 싸움으로 나누어 설명하고 있으나, 두 행위의 시간적·장소적 근접성 등에 비추어 볼 때 이는 망인이 단일 의사에 기해 행한 일련의 폭행과정의 일부로 평가되어야 할 것이다)의 업무관련성에 관해 살피건대, 앞서 처분의 경위에서 인정한 사실관계에 의하면, 망인이 하급자인 소외 2와 대화하는 도중에 갑자기 소외 2를 폭행하고, 동료 직원들의 만류에도 불구하고 위험한 물건(대걸레 막대기)까지 휴대하여 재차 소외 2에게 폭력을 행사한 점, 반면에 소외 2는 예상치 못한 망인의 도발에 당황하여 망인의 몸을 붙들고 뒹구는 등의 행위를 하였을 뿐, 적극적으로 주먹이나 발, 도구 등을 사용하여 망인을 공격하지는 않은 것으로 보이는 점, 원고는 소외 2가 먼저 갈취 등을 언급하며 망인을 도발하였다고 주장하나, 소외 2의 위와 같은 발언이 망인의 폭력을 정당화할 수 있을 정도로 지나쳤다고 보이지는 않는 점 등의 사정을 알 수 있다. 이러한 사정들을 종합하면 망인의 위와 같은 폭력행위는 사회적 상당성을 넘어서는 것으로서 사적인 화풀이의 일환이므로 이러한 경우까지 망인의 업무행위에 포함된다고 볼 수는 없고, 그 과정에서 발생한 스트레스 등으로 인하여 망인의 기존 질환이 악화되어 사망에 이르렀다고 하더라도, 이를 가리켜 망인의 업무에 내재하거나 이에 통상 수반하는 위험이 현실화된 것으로 평가할 수도 없다.      또한, 망인의 위와 같은 일련 행위는 형법 제260조에서 규정하고 있는 폭행죄의 구성요건에도 해당하므로, 범죄행위로 인한 근로자의 사망 등을 업무상 재해에서 배제하고 있는 산업재해보상보험법 제37조 제2항 본문이 적용되는 사안에도 해당한다.      따라서 망인의 사망은 산업재해보상보험법에 따른 유족급여 등의 지급대상이 되는 업무상 재해에 해당하지 아니한다고 할 것이므로 이 사건 처분은 적법하고, 이를 다투는 원고의 주장은 이유 없다.3. 결  론  그렇다면, 원고의 이 사건 청구는 이유 없으므로 이를 기각하기로 하여 주문과 같이 판결한다.[별지 생략]판사 강석규(재판장) 김유정 김대원  \n",
      "\n",
      "참조조문 \n",
      "  \n",
      "\n",
      "판시사항 :\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "==================================================================================================================\n",
      "==================================================================================================================\n",
      "사건 명 : 수입통관보류처분취소 (유사도: 91.47%)\n",
      "사건 번호 : 2021두46421 \n",
      "사건 종류 : 세무\n",
      "판례 일련번호 : 219833 \n",
      "법원 : 대법원 \n",
      "\n",
      "판례 내용 :\n",
      "【원고, 피상고인】 원고 (소송대리인 법무법인 대호 담당변호사 신동욱 외 1인)【피고, 상고인】 인천세관장【원심판결】 서울고법 2021. 6. 25. 선고 2020누46662 판결【주    문】  원심판결을 파기하고, 사건을 서울고등법원에 환송한다.【이    유】  상고이유를 판단한다.  1. 이 사건의 경위와 원심의 판단  원심판결 이유와 기록에 의하면 다음과 같은 사실을 알 수 있다.   가. 원고는 아래 나.항 기재와 같은 물품(이하 ‘이 사건 물품’이라고 한다)을 수입하기 위하여 수입신고를 하였다.  나. 이 사건 물품은 여성의 신체 외관을 본뜬 전신 인형 형태의 남성용 자위기구로서, 전체적으로 동양인의 피부색과 유사한 색의 실리콘 재질로 만들어져 있고, 앉거나 구부리는 등 다양한 자세가 가능하며, 머리 부분은 나사로 결합 및 분리가 가능하다. 이 사건 물품은 머리 부분에서 발 부분까지의 전체 길이가 150cm[원심이 인용한 제1심판결은 ‘머리를 제외한 크기는 약 150cm’라고 설시하였으나, 원고의 소장과 피고의 답변서 등에서 일치하여 ‘머리를 포함한 길이가 150cm’라고 주장하였고 관세청 심사청구 결정서(갑 제3호증의 1)에도 이와 같이 기재된 점 등에 비추어 보면, 위 설시는 오류로 보인다], 무게가 17.4kg이고, 얼굴 부분의 인상이 상당히 앳되게 표현되어 있다. 이 사건 물품의 항문 부분은 움푹 들어간 곳의 주위로 주름이 표현되어 있는 등 사람의 항문과 유사한 모습이고, 그 성기 부분은 성행위를 위하여 구멍이 뚫려 있고 음순과 질구가 표현되어 있는 등 여성의 성기 외관과 유사한 모습인데 음모 등은 표현되어 있지 않으며, 가슴과 엉덩이 부분만이 과장되게 표현되어 있다.   다. 피고는 2019. 10. 8. 이 사건 물품이 구 관세법(2019. 12. 31. 법률 제16838호로 개정되기 전의 것, 이하 ‘관세법’이라고만 한다) 제234조 제1호, 제237조 제3호에 해당한다는 이유로 이 사건 물품의 수입통관을 보류하는 처분(이하 ‘이 사건 처분’이라고 한다)을 하였다.  라. 원심은, 그 판시와 같은 사정을 들어 이 사건 물품을 전체적으로 관찰하여 볼 때 그 모습이 저속하고 문란한 느낌을 주지만 이를 넘어서서 사람의 존엄성과 가치를 심각하게 훼손·왜곡하였다고 평가할 수 있을 정도로 노골적인 방법에 의하여 성적 부위나 행위를 적나라하게 표현 또는 묘사한 것이라 볼 수 없다고 보아 수입통관을 보류한 이 사건 처분은 위법하다고 판단하였다.  2. 대법원의 판단  그러나 원심의 판단은 다음과 같은 이유로 수긍하기 어렵다.  가. 관세법 제234조 제1호는 ‘헌법질서를 문란하게 하거나 공공의 안녕질서 또는 풍속을 해치는 서적·간행물·도화, 영화·음반·비디오물·조각물 또는 그 밖에 이에 준하는 물품은 수출하거나 수입할 수 없다.’고 규정하고, 제237조 제3호는 ‘세관장은 이 법에 따른 의무사항을 위반하거나 국민보건 등을 해칠 우려가 있는 경우에는 해당 물품의 통관을 보류할 수 있다.’고 규정하고 있다. 관세법 제234조 제1호가 규정하는 ‘풍속을 해치는’이라고 함은, 특별한 사정이 없는 한 성풍속을 해치는 ‘음란성’을 의미하는 것으로 해석함이 상당하고(대법원 2009. 6. 23. 선고 2008두23689 판결 등 참조), 여기서 ‘음란’이라 함은 사회통념상 일반 보통인의 성욕을 자극하여 성적 흥분을 유발하고 정상적인 성적 수치심을 해하여 성적 도의관념에 반하는 것으로서, 존중·보호되어야 할 인격을 갖춘 존재인 사람의 존엄성과 가치를 심각하게 훼손·왜곡하였다고 평가할 수 있을 정도로 노골적인 방법에 의하여 성적 부위나 행위를 적나라하게 표현 또는 묘사한 것으로서, 음란 여부를 판단함에 있어서는 그 사회의 평균인의 입장에서 그 시대의 건전한 사회통념에 따라 객관적이고 규범적으로 평가하여야 한다(대법원 2008. 4. 11. 선고 2008도254 판결 등 참조). 우리 사회에서의 음란물에 대한 규제 필요성은 사회의 성윤리나 성도덕의 보호라는 측면을 넘어서 미성년자 보호 또는 성인의 원하지 않는 음란물에 접하지 않을 자유의 측면을 더욱 중점적으로 고려하여야 한다(대법원 2008. 3. 13. 선고 2006도3558 판결 참조).  나. 앞서 본 사실과 기록에 의하여 알 수 있는 다음과 같은 사정, 즉 이 사건 물품의 전체 길이, 무게는 16세 여성의 평균 신장, 체중에 현저히 미달하고, 얼굴 부분도 16세 미만 여성의 인상에 가까워 보이는 점, 이 사건 물품의 성기 부분은 여성의 성기 외관을 사실적으로 모사하면서도 음모의 표현이 없는 등 미성숙한 모습으로 보이는 점, 그 밖에 이 사건 물품의 형상, 재질, 기능, 용도 등에 비추어 보면, 이 사건 물품은 16세 미만 여성의 신체 외관을 사실적으로 본떠 만들어진 성행위 도구라고 볼 수 있다.  한편 청소년에게 음란한 행위를 조장하는 성기구 등 성 관련 물건은 청소년유해물건으로서, 19세 미만 청소년에게는 판매·대여·배포·무상제공이 금지되어 있으므로[청소년보호법 제2조 제4호 (나)목, 제28조 제1항, 제58조 제3호], 성행위 도구인 이 사건 물품은 19세 이상의 성인만이 구입하는 등 사용할 것을 예정하고 있다.  다. 1) 형법 제305조 제1항은 13세 미만의 사람에 대하여 간음 또는 추행을 한 자를 강간 등의 예에 의해 처벌하고, 같은 조 제2항은 13세 이상 16세 미만의 사람에 대하여 간음 또는 추행을 한 19세 이상의 자도 강간 등의 예에 의해 처벌하도록 규정하고 있다. 위 죄는 위계 또는 위력이나 폭행 또는 협박의 방법에 의함을 요하지 않으며, 설령 피해자의 동의가 있었더라도 성립한다(대법원 1982. 10. 12. 선고 82도2183 판결 참조). 즉, 19세 이상의 성인이 16세 미만 미성년자와 성행위를 하는 것은 그 자체로 형법상 처벌대상에 해당된다.  2) 「아동·청소년의 성보호에 관한 법률」(이하 ‘청소년성보호법’이라고 한다) 제2조 제5호는 종전의 ‘아동·청소년이용음란물’을 ‘아동·청소년성착취물’로 규정함으로써 아동·청소년을 대상으로 하는 음란물은 그 자체로 아동·청소년에 대한 성착취, 성학대를 의미하는 것임을 명확히 하고 있다.또한 실제의 아동·청소년뿐만 아니라 ‘아동·청소년으로 인식될 수 있는 사람이나 표현물’이 등장하는 경우도 아동·청소년성착취물에 포함되는바, 그 이유는 실제 아동·청소년인지와 상관없이 아동·청소년이 성적 행위를 하는 것으로 묘사하는 각종 매체물의 시청이 아동·청소년을 상대로 한 성범죄를 유발할 수 있다는 점을 고려하여 잠재적 성범죄로부터 아동·청소년을 보호하려는 데 있다(대법원 2019. 5. 30. 선고 2015도863 판결 참조). 가상의 표현물이라 하더라도 아동·청소년을 성적 대상으로 하는 표현물의 지속적 접촉은 아동·청소년의 성에 대한 왜곡된 인식과 비정상적 태도를 형성하게 할 수 있고, 또한 아동·청소년을 상대로 한 성범죄로 이어질 수 있다는 점을 부인하기 어렵다(헌법재판소 2015. 6. 25. 선고 2013헌가17, 24, 2013헌바85 전원재판부 결정 참조).   청소년성보호법은 그 외에도 제4조, 제5조에서 아동·청소년을 성적 착취와 학대 행위로부터 보호하기 위한 법적·제도적 장치의 마련 등을 국가의 의무로, 아동·청소년이 성범죄의 피해자가 되지 않도록 사회 환경을 정비하는 것을 사회의 책임으로 규정하고 있다.  라. 위와 같은 법리를 이 사건에 비추어 살펴보면, 이 사건 물품을 예정한 용도로 사용하는 것은 16세 미만 미성년자의 외관을 사실적으로 본뜬 인형을 대상으로 직접 성행위를 하는 것으로서, 이를 통해 아동을 성적 대상으로 취급하고 아동의 성을 상품화하며 폭력적이거나 일방적인 성관계도 허용된다는 왜곡된 인식과 비정상적 태도를 형성하게 할 수 있을뿐더러 아동에 대한 잠재적인 성범죄의 위험을 증대시킬 우려도 있다. 이 사건 물품은 그 자체가 성행위를 표현하지는 않더라도 직접 성행위의 대상으로 사용되는 실물이라는 점에서, 필름 등 영상 형태의 아동·청소년성착취물과 비교하여 그 위험성과 폐해를 낮게 평가할 수 없다.  마. 한편 이 사건 물품과 같이 사람의 신체 외관을 사실적으로 본떠 만들어진 성행위 도구가 16세 미만 미성년자의 신체 외관을 하였는지 여부는 구체적인 사안에서 당해 물품이 나타내고 있는 인물의 외관과 신체에 대한 묘사 등 여러 사정을 종합적으로 고려하여 개별적으로 판단하여야 한다.   바. 위와 같은 사정들을 종합하면, 이 사건 물품은 관세법 제234조 제1호가 규정한 ‘풍속을 해치는 물품’에 해당한다고 볼 여지가 있다. 따라서 원심으로서는 이 사건 물품의 형상, 재질, 기능, 용도, 이 사건 물품이 본뜬 인물의 외관과 신체에 대한 묘사 등을 확인하여 이 사건 물품이 16세 미만 미성년자의 신체 외관을 사실적으로 본뜬 성행위 도구에 해당하는지에 관하여 면밀히 심리한 다음, 이 사건 물품이 관세법 제237조 제3호, 제234조 제1호가 규정한 통관보류대상에 해당하는지 여부를 판단하였어야 한다. 그럼에도 원심은 이에 관하여 충분히 심리하지 않은 채 그 판시와 같은 이유만으로 이 사건 물품이 관세법 제234조 제1호가 규정한 ‘풍속을 해치는 물품’에 해당하지 않음을 전제로 이 사건 처분이 위법하다고 판단하였으니, 원심판결에는 관세법 제234조 제1호가 규정한 ‘풍속을 해치는 물품’에 관한 법리를 오해하고 필요한 심리를 다하지 아니함으로써 판결에 영향을 미친 잘못이 있다.   3. 결론   그러므로 나머지 상고이유에 대한 판단을 생략한 채 원심판결을 파기하고, 사건을 다시 심리·판단하도록 원심법원에 환송하기로 하여, 관여 대법관의 일치된 의견으로 주문과 같이 판결한다.대법관 천대엽(재판장) 조재연 민유숙(주심) 이동원  \n",
      "\n",
      "참조조문 \n",
      " [1] 구 관세법(2019. 12. 31. 법률 제16838호로 개정되기 전의 것) 제234조 제1호 / [2] 아동·청소년의 성보호에 관한 법률 제2조 제5호 / [3] 구 관세법(2019. 12. 31. 법률 제16838호로 개정되기 전의 것) 제234조 제1호, 제237조 제3호(현행 제237조 제1항 제3호 참조) \n",
      "\n",
      "판시사항 :\n",
      "[1] 구 관세법 제234조 제1호가 규정하는 ‘풍속을 해치는’의 의미와 판단 기준[2] 아동·청소년의 성보호에 관한 법률 제2조 제5호의 의미와 규정 취지[3] 甲이 여성의 신체 외관을 본뜬 전신 인형 형태의 남성용 자위기구를 수입신고 하였으나, 관할 세관장이 구 관세법 제234조 제1호 등에 해당한다는 이유로 위 물품의 수입통관을 보류하는 처분을 한 사안에서, 위 물품은 구 관세법 제234조 제1호가 규정한 ‘풍속을 해치는 물품’에 해당한다고 볼 여지가 있다고 한 사례\n",
      "\n",
      "\n",
      "\n",
      "==================================================================================================================\n",
      "==================================================================================================================\n",
      "사건 명 : 업무상과실치상 (유사도: 91.33%)\n",
      "사건 번호 : 2012도11361 \n",
      "사건 종류 : 형사\n",
      "판례 일련번호 : 173837 \n",
      "법원 : 대법원 \n",
      "\n",
      "판례 내용 :\n",
      "【피 고 인】 【상 고 인】 피고인들【변 호 인】 변호사 윤보은【원심판결】 서울중앙지법 2012. 8. 29. 선고 2012노1954 판결【주    문】피고인 2에 대한 원심판결을 파기하고, 이 부분 사건을 서울중앙지방법원 합의부에 환송한다.  피고인 1에 대한 공소를 기각한다.【이    유】   1. 피고인 1에 대하여   기록에 의하면 피고인 1은 검사의 이 사건 상고제기 이후인 2013. 5. 9. 사망한 사실이 인정되므로, 형사소송법 제382조, 제328조 제1항 제2호에 의하여 위 피고인에 대한 공소를 기각한다.   2. 피고인 2에 대하여   가. 이 사건 공소사실의 요지는 ‘피고인은 ○○○○개발 소속 이 사건 지하철 공사구간 현장안전업무를 담당하는 사람인바, 2008. 6.부터 이 사건 공사현장은 △△아파트사거리 교차로 상 횡단보도와 바로 인접해 설치되어 있고 기존의 횡단보도를 표시하는 도로 위 흰색표시가 완전히 지워지지 않은 채 흔적이 많이 남아 있는 상태였으며, 이 사건 공사현장에 H 강철빔(이하 ‘이 사건 강철빔’이라 한다)이 적재된 트럭이 있었고 이 사건 강철빔이 기존의 횡단보도 표시선 안쪽으로 돌출되어 있었음에도, 안전시설로 위 트럭 주위에 라바콘 3개만을 설치하고 차량통행 관리를 위한 신호수 1명만 세워 두었을 뿐 다른 특별한 조치를 취하지 아니한 업무상 과실로, 2010. 11. 3. 16:40 피해자 공소외 1(14세)이 위 횡단보도를 건너면서 이 사건 강철빔에 얼굴이 부딪혀 약 5주간의 치료가 필요한 폐쇄성 골절 등의 상해를 입게 하였다’는 것이다.   나. 원심은 제1심판결 이유를 인용하여, 기존의 횡단보도를 침범하여 돌출된 이 사건 강철빔이 방치된 이 사건 공사현장에 라바콘 3개를 세워 두고 신호수 1명을 배치한 것만으로는 안전사고예방을 위하여 충분한 주의의무를 다하였다고 볼 수 없고, 비록 흔적이 남아 있던 기존의 횡단보도를 따라 무단횡단을 하던 피해자에게도 전방을 제대로 살피지 않은 채 부주의하게 보행한 과실이 있어 그것도 이 사건 사고 발생의 한 원인이 되었다고 하더라도, 피고인의 주의의무 위반이 이 사건 사고 발생에 대한 유력한 원인이 된 이상 그 주의의무 위반과 상해의 결과 사이에 인과관계가 있다는 이유로 제1심의 유죄판결을 그대로 유지하였다.   다. 그러나 원심의 위와 같은 판단은 기록에 의하여 알 수 있는 다음과 같은 사정에 비추어 볼 때 쉽게 수긍할 수 없다.   ① 이 사건 사고 발생 당시 이 사건 강철빔 위에서 작업하던 공소외 2는 일관되게 피해자가 횡단보도를 건너기 전까지 왼손으로 책을 들고 읽으며 오다가 횡단보도 보행자 신호가 얼마 남지 않은 상황에서 기존의 횡단보도가 시작되는 지점의 오른쪽에 길게 설치된 가드레일을 뛰어넘은 다음 팔을 굽혀 책을 든 자세 그대로 이 사건 강철빔 방향으로 달려왔다고 진술하였고, 피해자도 만화책을 읽으면서 횡단보도 방향으로 가다가 횡단보도에 다다르기 얼마 전에 보행자 신호가 15초 정도 남은 것을 보고 급하게 뛰어가다가 이 사건 강철빔을 미처 발견하지 못하고 부딪혔다고 진술하였는바, 이에 비추어 보면 피해자가 책을 읽으면서 걸어가던 중 보행자 신호가 얼마 남지 않은 상황에서 급히 횡단보도를 뛰어 건너려다가 주변을 제대로 살피지 않았을 가능성이 크다.   ② 한편, 피해자는 위 가드레일이 있는 쪽에서 건너기 시작한 것이 아니라 청담역에서 나와 언덕을 내려오다가 횡단보도의 왼쪽 부분에서부터 건너기 시작하였다는 취지로 진술하였다. 그러나 이 사건 강철빔이 적재된 트럭과 라바콘이 설치되어 있는 바닥 부분에는 기존의 횡단보도 표시선 흔적이 별로 남아 있지 않았던 점, 라바콘과 위 트럭 사이를 지나게 되면 그다음부터는 차량이 신호 대기 중인 지점으로서 그 바닥에 선명하게 차량 정지선이 그어져 있는 점, 피해자가 말한 횡단보도 진입 지점에서 출발하여 라바콘과 이 사건 강철빔이 적재된 트럭 사이를 지나는 것은 횡단보도를 건너는 최단거리의 직진 경로가 아닌 점 등에 비추어 볼 때 피해자가 진술한 방향으로 횡단보도를 건너는 것은 그 이동 경로가 부자연스럽고, 그와 같이 건너는 사람으로서는 횡단보도가 아닌 부분을 통과하여 건넌다는 사실을 충분히 알 수 있었을 것이다.   ③ 위 공소외 2와 이 사건 사고 당시 사고 장소에서 신호수 업무를 보았던 공소외 3의 진술에 의하면 이 사건 강철빔 끝에 묶여진 안전띠가 바닥까지 늘어뜨려져 있었던 사실을 알 수 있고, 그 밖에 이 사건 강철빔 주변에는 3개의 라바콘이 놓여 있었고 신호수 1명도 배치되어 있었던 점, 이 사건 강철빔이 적재된 트럭과 공사장의 위치, 작업 상황, 라바콘이 놓인 지점 및 라바콘과 위 트럭과의 간격 등에 비추어 볼 때 횡단보도를 건너는 보행자로서는 라바콘과 위 트럭 사이를 지나가는 것이 위험하다는 것을 충분히 인지할 수 있는 상황이었다고 보인다.   ④ 피해자는 평소 통학하면서 이 사건 도로를 지나다니기 때문에 이 사건 사고 장소에서 공사가 진행 중인 사실을 잘 알고 있었다고 진술하였다.   ⑤ 피고인이 관련 법령이나 내부 규칙 등에서 정하고 있는 안전조치를 제대로 이행하지 않았다고 볼 만한 증거가 없다.   이러한 사정에 비추어 보면, 피고인이 안전조치를 취하여야 할 업무상 주의의무를 위반하였다고 보기 어렵고, 일부 도로 지점에서 기존의 횡단보도 표시선이 제대로 지워지지 않고 드러나 있었다거나 라바콘을 3개만 설치하고 신호수 1명을 배치하는 외에 별다른 조치를 취하지 아니하였다고 하더라도 그것과 이 사건 사고 발생 사이에 상당인과관계에 있다고 보기도 어렵다 할 것임에도, 원심이 이와 달리 판단한 것은 필요한 심리를 다하지 아니하거나 업무상과실치상죄의 업무상 주의의무 또는 상당인과관계에 관한 법리를 오해하여 판단을 그르친 것이다.   3. 결론   그러므로 피고인 2에 대한 원심판결을 파기하고, 이 부분 사건을 다시 심리·판단하게 하기 위하여 원심법원에 환송하며, 피고인 1에 대한 공소는 기각하기로 하여, 관여 대법관의 일치된 의견으로 주문과 같이 판결한다.대법관 박보영(재판장) 민일영(주심) 이인복 김신  \n",
      "\n",
      "참조조문 \n",
      "형법 제268조 \n",
      "\n",
      "판시사항 :\n",
      " 지하철 공사구간 현장안전업무 담당자인 피고인이 공사현장에 인접한 기존의 횡단보도 표시선 안쪽으로 돌출된 강철빔 주위에 라바콘 3개를 설치하고 신호수 1명을 배치하였는데, 피해자가 위 횡단보도를 건너면서 강철빔에 부딪혀 상해를 입은 사안에서, 제반 사정에 비추어 피고인이 안전조치를 취하여야 할 업무상 주의의무를 위반하였다고 보기 어려운데도, 이와 달리 보아 업무상과실치상죄를 인정한 원심판결에 법리오해 등의 잘못이 있다고 한 사례.\n",
      "\n",
      "\n",
      "\n",
      "==================================================================================================================\n",
      "==================================================================================================================\n",
      "사건 명 : 정보통신망이용촉진및정보보호등에관한법률위반(명예훼손) (유사도: 91.32%)\n",
      "사건 번호 : 2016고정3950 \n",
      "사건 종류 : 형사\n",
      "판례 일련번호 : 185129 \n",
      "법원 : 서울중앙지방법원 \n",
      "\n",
      "판례 내용 :\n",
      "【피 고 인】 【검    사】 이재원 외 1인【변 호 인】 법무법인 명진 담당변호사 임대원 외 2인【주    문】피고인 1을 벌금 300만 원, 피고인 2를 벌금 200만 원에 각 처한다.  피고인들이 위 각 벌금을 납입하지 아니하는 경우에는 10만 원을 1일로 환산한 기간 피고인들을 노역장에 유치한다.  피고인들에 대하여 위 각 벌금에 상당한 금액의 가납을 명한다.【이    유】【범죄사실】  1. 기초 사실피고인 1은 게시판형 커뮤니티 서비스 등을 제공하는 커뮤니티 포털사이트인 ‘공소외 1 주식회사’(이하 ‘공소외 1 회사’)의 부사장, 피고인 2는 위 공소외 1 회사의 전략사업팀 팀장으로 각각 근무하는 사람이고, 피해자 ○○ 주식회사는 전자상거래 쇼핑몰인 일명 ‘○○’을 운영하는 회사이다.  2. 피고인 1  피고인은 2015. 1. 12. 15:38경 서울 강남구 (주소 생략)공소외 1 회사 사무실에서, 컴퓨터를 이용하여 인터넷 ‘△△△ △△’ 사이트(인터넷주소 생략) 러브스토리 게시판에 『소셜커머스 총체적 난국이네요』라는 제목으로 “○x 관련 퍼온 글인데 한두 업체의 문제가 아닌가보네요.. 작년말까지 일하고 문자 한통으로 해고 당했군요 나참 어이가 없는.. 처음에 일하게 된 게기는 열심히 하면 정규직도 가능하다는 말이 있었고 나름 이름되는 회사에서 일해보고 싶다는 생각이 들었거든요. 근데 ㅅㅂ 아침 8시에 출근해서 퇴근을 하려면 기본으로 밤 11시가 넘음 처음에는 8시퇴근이라고 되어 있었는데 열심히 해야 정규직이 된다는 생각하여 한마디도 불평을 이야기 못했음 점심시간도 빵으로 때워가며 일했고 하루가 정말 고되고 힘들었음 6개월 뒤에 부푼 기대를 갖고 있었으나 저는 해고처리.. 내가 잘못한게 있었나? 사고 한번도 안 났고 배송 밀린 것도 없었는데.. 없었던게 아니라 없게 할려고 밤을 지셌는데 처음에는 저보다 일을 잘 한사람이 좀 더 많았겠지라고 생각했는데 알고보니 합격한 사람은 아무도 없습니다. 그냥 계약종료로 전부 퇴사처리되었네요ㅜㅜ 그것도 문자1통받고 뭐라고 말도 못했습니다. 차량반납하고 끝.. 이런게 갑질이구나 난 당했구나.”라는 내용의 글을 작성하여 이를 게시하였다.  그러나 사실은 피해자는 문자통보가 아닌 피평가자 면담을 통하여 계약종료 절차를 진행하고, 6개월의 계약기간 종료 후 특별한 사유가 없으면 기존 근로자들과 계약을 연장하거나 정규직 전환을 해왔다.  이로써 피고인은 피해자를 비방할 목적으로 정보통신망을 통하여 공공연하게 거짓의 사실을 드러내어 피해자의 명예를 훼손하였다.  3. 피고인 2  피고인은 2015. 1. 12. 16:08경 위 공소외 1 회사 사무실에서, 피고인의 휴대전화를 이용하여 위 ‘△△△ △△’ 사이트 자유게시판에 『[펌] ○○맨을 살려주세요』라는 제목으로 “어제 ○○맨에 대한 네이버 기사를 보고 ○○맨이었던 제 남편이 생각나더군요. 감성배송? 고객만족? 댓글들을 보니 여전하더군요.. 일반 택배보다 못한 처우와 대우와 급여 이게 과연 회사입니까? 열시 열한시 까지 배송하고 월급250만원이 말이나 됩니까? 점심조차 먹을 시간도 없고 저녁식대 조차 제대로 지켜지지 않는 그런 환경에서 고객만족이라는게 말이나 되는지 묻고 싶습니다. 어떻게 힘들게 죽도록 고생하는 직원들의 말에는 귀를 닫고 모른 체 하십니까.. 오전 8시 출근에 11시, 12시 퇴근이 말이 되나요? □□배송? 그게 누구를 위한 겁니까? 계약직으로 6개월씩 연장만 해대며 정규직 전환율은 0%가 말이됩니까? 계약직은 누구말대로 회사의 노예며, 시키면 시키는대로 아니면 잘리고 그런건가요? 채용공고에는 버젓이 출퇴근 시간을 8시부터 8시로 기재놓고 노동법위반은 아닙니까? 불쌍한 ○○맨을 위해 제발 목소리를 높여주세요..”라는 내용의 글을 작성하여 이를 게시하고, 계속해서 같은 날 16:11경 인터넷 ‘◇◇◇’ 사이트에 위와 같은 내용의 글을 게시하였다.  그러나 사실은 피해자는 저녁식대, 야근수당, 주말수당 등을 포함하여 급여(기타 수당)를 지급하고 정규직 전환율은 0%가 아니었다.  이로써 피고인은 피해자를 비방할 목적으로 정보통신망을 통하여 공공연하게 거짓의 사실을 드러내어 피해자의 명예를 훼손하였다.【증거의 요지】  1. 제1회 공판조서 중 피고인들의 일부 진술기재  1. 피고인들에 대한 검찰 피의자신문조서  1. 각 수사보고  1. 각 캡처사진, ○○맨 계약연장절차, ○○맨 입·퇴사 현황, 근로계약서 5부, 2014년 12월 정기급여명세서 3부【법령의 적용】  1. 범죄사실에 대한 해당법조 및 형의 선택  피고인들: 정보통신망 이용촉진 및 정보보호 등에 관한 법률 제70조 제2항, 벌금형 선택  1. 경합범가중피고인 2: 형법 제37조 전단, 제38조 제1항 제2호, 제50조  1. 노역장유치  피고인들: 형법 제70조 제1항, 제69조 제2항  1. 가납명령  피고인들: 형사소송법 제334조 제1항【피고인들과 변호인의 주장에 관한 판단】  1. 주장의 요지  가. 피고인들은 인터넷 포털사이트에 게재된 제3자의 글을 단순 전재하였을 뿐이므로, 피해자에 대한 명예훼손행위를 한 것이 아니다.  나. 피고인 1이 전재한 글 중 문제 되는 (1) ‘문자 한 통으로 해고 당했군요.’ (2) ‘알고보니 합격한 사람은 아무도 없습니다. 그냥 계약종료로 전부 퇴사처리되었네요.’ 부분이 허위라는 점이 입증되지 아니하였다.  다. 피고인 2가 전재한 글 중 문제 되는 (1) ‘일반 택배보다 못한 처우와 대우와 급여’, (2) ‘열시 열한시까지 배송’, (3) ‘월급 250만 원’, (4) ‘점심조차 먹을 수 시간도 없고’, (5) ‘저녁식대 조차 제대로 지켜지지 않는’, (6) ‘11시, 12시 퇴근이 말이 되나요? □□배송? 그게 누구를 위한 것입니까? 계약직으로 6개월씩’, (7) ‘정규직 전환율은 0%’ 부분에 관하여, (1)은 사실을 기재한 것이 아니라 글쓴이의 주관적 판단이고, (2) 내지 (7)은 허위라는 점이 입증되지 아니하였다.  라. 피고인들은 전재한 글의 내용이 진실이라고 믿었고, 그렇게 믿을 만한 상당한 이유가 있으므로, 피고인들의 행위는 형법 제310조에 의하여 위법성이 조각된다.  2. 판단  가. 제1의 가.항의 주장에 대하여  인터넷에 제3자의 표현물을 게시한 행위가 전체적으로 보아 단순히 그 표현물을 인용하거나 소개하는 것에 불과한 경우에는 명예훼손의 책임이 부정되고, 제3자의 표현물을 실질적으로 이용·지배함으로써 제3자의 표현물과 동일한 내용을 직접 적시한 것과 다름없다고 평가되는 경우에는 명예훼손의 책임이 인정된다(헌법재판소 2013. 12. 26. 선고 2009헌마747 전원재판부 결정 참조).  판시 각 증거에 의하여 인정할 수 있는 다음과 같은 사정 즉, ① 피고인들은 원 게시글의 출처를 정확히 밝히지 아니한 채 판시 각 글을 작성한 점, ② 피고인들은 원 게시글에 관한 인터넷 주소를 링크걸거나 소개하는 방식이 아니라 원 게시물의 내용을 새로운 게시물의 형태로 작성하였던 점 등을 고려하면, 설령 피고인들이 원 게시글을 전재한 것에 불과하더라도, 피고인들의 행위는 원 게시글을 인용하거나 소개하는 것을 넘어서서 판시 각 글을 직접 적시한 것과 다름없다고 보이므로, 피해자에 대한 명예훼손행위 책임을 부정할 수 없다.  나. 제1의 나.항의 주장에 대하여  판시 각 증거에 의하여 인정할 수 있는 다음과 같은 사정 즉, ① 피해자는 절차에 따라 수습직원을 면담한 후 재계약 여부를 결정하고 있고, 수습기간 중에 있는 직원을 귀책사유 없이 해고한 사실이 없으며, 피고인 1의 게시글과 같이 절차를 거치지 아니하고 문자메시지 1통으로 소속 직원을 해고한 사실이 없으므로, (1)의 내용은 허위인 점, ② ○○맨 입·퇴사 현황에 따르면, 2014. 12. 1. 기준으로 한 입사인원 대비 계약만료율은 6.8%에 불과하고, 각 입사일별 수습직원들을 기준으로 보아도 계약만료율은 최대 13.1%에 불과하므로(각 입사일별로 수습직원들이 전부 계약종료로 퇴사처리된 사실이 없다), (2)의 내용 역시 허위인 점 등을 고려하여 볼 때, 피고인 1이 작성한 글의 내용이 전부 허위인 사실을 인정할 수 있다.  다. 제1의 다.항의 주장에 대하여  판시 각 증거에 의하여 인정할 수 있는 다음과 같은 사정 즉, ① (1)의 내용은 피해자 소속 직원들의 처우와 대우, 급여가 일반 택배회사보다 낮다는 사실을 적시한 것으로서 단순한 가치판단이라고 보기 어려운 점, ② (1) 내지 (6)의 내용은 게시글의 전체적인 내용을 볼 때, 피해자의 직원들이 밤 늦게까지 근무하더라도 저녁식대조차 별도로 지급받지 못한 채 기본급만을 받고 있다는 사실을 적시한 것인데, 피해자는 직원들에게 기본급 외에도 시간외근로수당, 야근수당, 휴일근로수당뿐만 아니라 저녁식대까지 별도로 지급한 사실이 있으므로, (1) 내지 (6)의 내용은 허위인 점, ③ 피해자는 직원들의 정규직 전환을 규정에 따라 하여 왔으므로, (7)의 내용은 허위인 점 등을 고려하면, 피고인 2가 작성한 각 글의 내용 역시 모두 허위인 사실을 인정할 수 있다.  라. 제1의 라.항의 주장에 대하여형법 제307조 제2항의 허위사실 적시에 의한 명예훼손죄에서 적시된 사실이 허위인지 여부를 판단함에 있어서는 적시된 사실의 내용 전체의 취지를 살펴볼 때 세부적인 내용에서 진실과 약간 차이가 나거나 다소 과장된 표현이 있는 정도에 불과하다면 이를 허위라고 볼 수 없으나, 중요한 부분이 객관적 사실과 합치하지 않는다면 이를 허위라고 보아야 한다. 나아가 행위자가 그 사항이 허위라는 것을 인식하였는지 여부는 성질상 외부에서 이를 알거나 증명하기 어려우므로, 공표된 사실의 내용과 구체성, 소명자료의 존재 및 내용, 피고인이 밝히는 사실의 출처 및 인지 경위 등을 토대로 피고인의 학력, 경력, 사회적 지위, 공표 경위, 시점 및 그로 말미암아 예상되는 파급효과 등의 여러 객관적 사정을 종합하여 판단할 수밖에 없으며, 범죄의 고의는 확정적 고의뿐만 아니라 결과 발생에 대한 인식이 있고 그를 용인하는 의사인 이른바 미필적 고의도 포함하므로 허위사실 적시에 의한 명예훼손죄 역시 미필적 고의에 의하여도 성립하고(대법원 2014. 3. 13. 선고 2013도12430 판결 등 참조), 위와 같은 법리는 정보통신망 이용촉진 및 정보보호 등에 관한 법률 제70조 제2항 소정의 허위사실 적시에 의한 명예훼손죄의 판단에서도 마찬가지로 적용된다고 봄이 타당하다.  또한 정보통신망 이용촉진 및 정보보호 등에 관한 법률 제70조 제2항 소정의 ‘사람을 비방할 목적’이란 가해의 의사 내지 목적을 요하는 것으로서, 사람을 비방할 목적이 있는지 여부는 당해 적시 사실의 내용과 성질, 당해 사실의 공표가 이루어진 상대방의 범위, 그 표현의 방법 등 그 표현 자체에 관한 제반 사정을 감안함과 동시에 그 표현에 의하여 훼손되거나 훼손될 수 있는 명예의 침해 정도 등을 비교, 고려하여 결정하여야 한다(대법원 2006. 8. 25. 선고 2006도648 판결 등 참조).  위와 같은 법리를 토대로 이 사건에 관하여 보건대, 판시 각 증거에 의하여 인정할 수 있는 다음과 같은 사정 즉, ① 피고인들은 국내 최대 커뮤니티 인터넷 웹사이트의 하나인 공소외 1 회사의 임직원으로 해당 직업의 업무상 인터넷에 허위의 게시물이 적지 않게 올라오고 있는 사실을 충분히 인식하고 있었을 것으로 보이는 점, ② 그럼에도 피고인들은 별도의 사실확인 없이 원 글의 출처도 생략한 채 판시 각 글을 작성한 점, ③ 피고인들이 판시 각 글을 작성한 시점에는 그 내용에 관하여 피해자에 대한 구체적인 의혹이 있거나, 공식적인 언론보도도 있지 아니한 점(피고인들의 행위가 있은 날로부터 2~3일이 경과된 이후에서야 피고인들이 게시한 글 내용의 진실공방을 소개하는 형태의 인터넷 기사가 일부 나왔을 뿐이다), ④ 피고인들이 작성한 판시 각 글의 내용은 피해자가 근로자들을 착취하는 비도덕적인 기업으로 일방적으로 매도하는 내용인 점, ⑤ 판시 각 글이 게재된 사이트들은 대형 커뮤니티 웹사이트들로서 판시 각 글의 내용이 수많은 인터넷 이용자들에게 광범위하게 전파된 것으로 보이는 점 등을 종합하면, 피고인들에게 적어도 허위의 사실을 게재한다는 점에 대한 미필적인 고의가 있었고, 또한 피해자를 비방할 목적이 있었다고 봄이 타당하다.  이와 같이 피고인들의 비방의 목적이 인정되는 정보통신망을 통한 허위사실 적시 명예훼손행위에는 위법성 조각에 관한 형법 제310조가 적용될 수 없다(대법원 2013. 2. 28. 선고 2010도14037 판결 등 참조).  마. 결론  따라서 피고인들과 변호인의 위 주장은 모두 받아들이기 어렵다.【양형의 이유】  피고인들의 범행으로 인하여 피해자의 사회적 평가가 상당히 훼손된 것으로 보이는 점, 피해자가 피고인들의 처벌을 적극 바라고 있는 점, 그 밖에 피고인들의 연령, 성행, 환경 등 변론에 나타난 양형의 조건이 되는 여러 사정을 참작하여 보면, 이 사건 약식명령에서 정한 각 벌금형이 부당하다고 보이지 아니한다. 따라서 주문과 같이 피고인들에 대한 형을 정한다.판사 박강민  \n",
      "\n",
      "참조조문 \n",
      "형법 제13조, 정보통신망 이용촉진 및 정보보호 등에 관한 법률 제70조 제2항 \n",
      "\n",
      "판시사항 :\n",
      "  인터넷 커뮤니티 포털사이트를 운영하는 甲 주식회사의 임직원인 피고인들이, ‘전자상거래 쇼핑몰 업체인 乙 주식회사는 배송 근로자들을 착취하는 비도덕적인 기업’이라는 취지로 제3자가 인터넷상에 게재한 허위 내용의 글을 옮겨와 다른 인터넷 사이트 게시판에 작성·게시함으로써 乙 회사의 명예를 훼손하였다고 하여 정보통신망 이용촉진 및 정보보호 등에 관한 법률 위반으로 기소된 사안에서, 피고인들에게 적어도 허위의 사실을 게재한다는 점에 대한 미필적인 고의가 있었고, 乙 회사를 비방할 목적도 있었다는 이유로 유죄를 선고한 사례\n",
      "\n",
      "\n",
      "\n",
      "==================================================================================================================\n",
      "==================================================================================================================\n",
      "사건 명 : 유족급여및장의비부지급처분취소(업무상 재해 여부가 문제된 사건) (유사도: 91.31%)\n",
      "사건 번호 : 2016두55919 \n",
      "사건 종류 : 일반행정\n",
      "판례 일련번호 : 184748 \n",
      "법원 : 대법원 \n",
      "\n",
      "판례 내용 :\n",
      "【원고, 상고인】 【피고, 피상고인】 근로복지공단【원심판결】 서울고법 2016. 10. 12. 선고 2016누40353 판결【주    문】  원심판결을 파기하고, 사건을 서울고등법원에 환송한다.【이    유】  상고이유를 판단한다.  1. 상고이유 제1점에 대하여  가. 산업재해보상보험법(이하 ‘산재보험법’이라 한다) 제5조 제1호는 “업무상의 재해란 업무상의 사유에 따른 근로자의 부상·질병·장해 또는 사망을 말한다.”라고 규정하고 있는 바, 근로자가 타인의 폭력에 의하여 재해를 입은 경우라고 하더라도, 가해자의 폭력행위가 피해자와의 사적인 관계에서 기인하였다거나 피해자가 직무의 한도를 넘어 상대방을 자극하거나 도발함으로써 발생한 경우에는 업무기인성을 인정할 수 없어 업무상 재해로 볼 수 없다고 할 것이나, 그것이 직장 안의 인간관계 또는 직무에 내재하거나 통상 수반하는 위험이 현실화되어 발생한 것으로서 업무와 사이에 상당인과관계가 있으면 업무상 재해로 인정하여야 할 것이다(대법원 1995. 1. 24. 선고 94누8587 판결 등 참조).  나. 원심은 다음과 같은 사실을 인정하였다.  (1) 원고의 배우자인 소외 1(이하 ‘망인’이라 한다)은 주식회사 △△의 □□공장 생산팀 제병C조의 반장이었고, 소외 2는 같은 조에 속한 후배 직원으로 금전관리 등 총무 업무를 하고 있었다.  (2) 망인은 야간근무 중이던 2014. 7. 16. 22:00경 회사로부터 지급받은 야식비의 사용 방법을 두고 소외 2와 의견을 나누던 중 말다툼을 하게 되었다.  (3) 위 말다툼이 격화되어 소외 2가 망인에게 ‘야식비를 회식 불참자에게 나누어 주지 않으면 이는 엄연히 갈취나 마찬가지이다’라는 취지의 발언을 하기에 이르렀고, 이에 격분한 망인이 소외 2의 얼굴을 때리면서 몸싸움이 시작되어 두 사람은 서로 엉겨 붙은 채 바닥을 수차례 구르기도 하였다. 동료 직원들의 만류로 몸싸움이 잠시 중단되었으나 망인이 다시 대걸레 막대기를 들고 소외 2에게 휘두르면서 두 사람이 다시 엉겨 붙어 싸우게 되었다. 동료 직원들이 다시 몸싸움을 말리고 만류하는 과정에서 망인은 갑자기 기력을 잃고 잠시 걸어 나가다가 그대로 쓰러졌다(이하 망인이 쓰러지기까지의 과정을 통틀어 ‘이 사건 다툼’이라 한다).  (4) 망인은 곧바로 병원으로 이송되었으나 이 사건 다툼이 있은 지 얼마 지나지 아니한 2014. 7. 17. 00:33경 급성 심장사를 원인으로 사망하였다.  다. 원심은 이러한 사실관계를 전제로 하여, 평소 심장질환이 있던 망인이 이 사건 다툼의 과정에서 받은 충격으로 인해 사망에 이르게 되었다는 점은 인정하면서도, 다른 한편으로 ① 망인이 먼저 소외 2를 폭행하였고 동료 직원들의 만류에도 불구하고 재차 소외 2에게 폭력을 행사한 점, ② 반면 소외 2는 적극적으로 망인을 공격하지는 않은 점, ③ 소외 2의 갈취 관련 발언이 망인의 선행 폭력을 정당화할 수 있을 정도로 지나친 것으로 보이지는 않는 점 등의 사정을 들어, 이 사건 다툼은 망인의 사적인 화풀이의 일환으로 망인의 업무행위에 포함된다고 볼 수 없고, 따라서 이로 인하여 망인의 심장질환이 악화되어 사망에 이르렀다고 하더라도 이를 업무상 재해로 평가할 수 없다고 판단하였다.  라. 그러나 원심의 이러한 업무관련 \n",
      "\n",
      "참조조문 \n",
      "[1] 산업재해보상보험법 제5조 제1호 / [2] 산업재해보상보험법 제37조 제2항 \n",
      "\n",
      "판시사항 :\n",
      "[1] 근로자가 타인의 폭력에 의하여 재해를 입은 경우, 업무상 재해로 인정할 수 있는지 판단하는 기준[2] 산업재해보상보험법 제37조 제2항에서 규정하고 있는 ‘근로자의 범죄행위가 원인이 되어 사망 등이 발생한 경우’의 의미\n",
      "\n",
      "\n",
      "\n",
      "==================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from konlpy.tag import Komoran\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# colab 사용 시 /content/ 내 작업공간 사용 시 data/ /content/drive/MyDrive/data/\n",
    "file_path = 'data/'\n",
    "\n",
    "# 데이터, 모델, 토크나이저 로드\n",
    "tokenizer = joblib.load(f'{file_path}tokenizer.pkl')\n",
    "model = AutoModel.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
    "model.load_state_dict(torch.load(f'{file_path}kc_electra_model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Komoran 객체 생성\n",
    "komoran = Komoran()\n",
    "\n",
    "embeddings = joblib.load(f'{file_path}embeddings.pkl')\n",
    "kmeans = joblib.load(f'{file_path}kmeans_model.pkl')\n",
    "df = pd.read_pickle(f'{file_path}data_with_preprocessed_text.pkl')\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def prepro_text(text):\n",
    "    tokens = komoran.morphs(text)\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# 텍스트 임베딩 생성 함수\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embedding\n",
    "\n",
    "# 판례 추천 함수\n",
    "def recommend_court(query, top_k=5):\n",
    "    preprocessed_query = prepro_text(query)\n",
    "    query_embedding = embed_text(preprocessed_query)\n",
    "    \n",
    "    # 유사도 계산\n",
    "    similarities = cosine_similarity(query_embedding, embeddings).flatten()\n",
    "    \n",
    "    # 유사도 기준으로 상위 top_k 개 판례 선택\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    recommended_courts = df.iloc[top_indices]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        result = (\n",
    "            df.iloc[idx]['event_Title'],\n",
    "            df.iloc[idx]['event_num'],\n",
    "            df.iloc[idx]['event_type'],\n",
    "            df.iloc[idx]['jPrecedent_num'],\n",
    "            df.iloc[idx]['court_name'],\n",
    "            df.iloc[idx]['court_detail'],\n",
    "            df.iloc[idx]['court_reference'],\n",
    "            df.iloc[idx]['court_Decision'],\n",
    "            similarities[idx] * 100  # 유사도를 퍼센트로 변환\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 사용자 질의 입력\n",
    "query = input(\"상황을 작성해 주세요 (자세히 작성 시 검색에 도움이 됩니다.) :\\n\")\n",
    "recommended_courts = recommend_court(query)\n",
    "\n",
    "if isinstance(recommended_courts, str):\n",
    "    print(recommended_courts)\n",
    "else:\n",
    "    court_detail_OX = input(\"판례 및 판시 사항을 보시겠습니까? (Y/N): \\n\")\n",
    "    for event_Title, event_num, event_type, jPrecedent_num, court_name, court_detail, court_reference, court_Decision, similarity in recommended_courts:\n",
    "        print('==================================================================================================================')\n",
    "        if (court_detail_OX == 'N') or (court_detail_OX == 'n'):\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\n판례 일련번호 : {jPrecedent_num} \\n법원 : {court_name} \\n\\n참조조문 \\n{court_reference}\\n\\n\\n\")\n",
    "        else:\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\n판례 일련번호 : {jPrecedent_num} \\n법원 : {court_name} \\n\\n판례 내용 :\\n{court_detail} \\n\\n참조조문 \\n{court_reference} \\n\\n판시사항 :\\n{court_Decision}\\n\\n\\n\")\n",
    "        print('==================================================================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e137f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상황을 작성해 주세요 (자세히 작성 시 검색에 도움이 됩니다.) :\n",
      "이탈거래처에 대하여 유지보수작업을 마친 후 그 작업에 따라 변경된 원본 프로그램을 원고의 파일서버에 저장하지 아니하였거나, 퇴사 당시 원고에게 그들이 사용하던 원고의 영업자료와 원본 프로그램이 저장된 원고 소유의 노트북을 반납하지 아니하였다 하더라도, 원고의 웹하드에 원고의 이탈거래처의 변경된 원본 프로그램이 저장되어 있었고, 원고의 이탈거래처 서버에도 원본 프로그램 및 실행파일 등이 저장되어 있었던 것으로 보이는 이상, 그와 같은 사정만으로 피고들이 원고의 이탈거래처에 대한 유지보수작업을 방해하였다고\n",
      "판례 및 판시 사항을 보시겠습니까? (Y/N): \n",
      "y\n",
      "==================================================================================================================\n",
      "사건 명 : 건물등철거·소유권확인 (유사도: 94.27%)\n",
      "사건 번호 : 2013다43666 \n",
      "사건 종류 : 민사\n",
      "판례 일련번호 : 172481 \n",
      "법원 : 대법원 \n",
      "\n",
      "판례 내용 :\n",
      "【원고(반소피고), 피상고인】 사회복지법인 광주여자기독교청년회복지사업위원회 (소송대리인 법무법인 바른길 담당변호사 김용일 외 4인)【피고(반소원고), 상고인】 【원심판결】 광주지법 2013. 5. 15. 선고 2012나13701, 13718 판결【주    문】  원심판결을 파기하고, 사건을 광주지방법원으로 환송한다.【이    유】  상고이유에 대하여 판단한다.  1. 원심판결 이유에 의하면, 원심은 소외 1이 1966. 7. 1.경부터 이 사건 (지번 1 생략) 대지 중 판시 (다), (라)부분을 점유해 오다가 2003. 3. 21.경부터 위 부분을 피고 1이 점유해 온 사실과, 소외 2가 1966. 8. 30.경부터 이 사건 (지번 1 생략) 대지 중 판시 (나)부분을 점유하다가 소외 3, 4, 5 등을 거쳐 소외 6에게 그 부분 점유가 차례로 승계되었고 그 후 피고 2가 소외 6의 상속인으로서 그 점유를 승계받은 사실을 인정하고도, 제1심증인 소외 7의 증언 등에 의하면 1974. 10. 14.경 이 사건 (지번 1 생략) 대지에 관하여 경계측량이 이루어졌는데 이때 소외 1이 참가하여 판시 (다), (라)부분이 원고의 소유임을 확인한 사실을 인정할 수 있으므로 소외 1의 판시 (다), (라)부분에 관한 점유는 그 무렵 타주점유로 전환되었고, 제1심증인 소외 8의 증언과 사실조회 결과 등에 의하면 1992. 6. 15.경 이 사건 (지번 1 생략) 대지에 관하여 경계측량이 이루어졌는데 이때 소외 6이 위 측량과정에 참여하여 판시 (나)부분이 원고의 소유임을 확인하고 이를 반환하기로 한 사실 및 소외 6의 처인 피고 1이 2002. 9. 30. 이 사건 (지번 1 생략) 대지에 인접한 대지 상에 건물을 신축할 때 건축신고한 도면에는 판시 (나)부분이 그 부지에 포함되어 있지 않은 사실을 인정할 수 있으므로 소외 6은 1992. 6. 15.경 판시 (나)부분에 관한 취득시효 이익을 포기하였다고 봄이 상당하다고 판단하였다.  2. 그러나 원심의 위와 같은 판단은 다음과 같은 이유로 수긍할 수 없다.   가. 점유의 시초에 자신의 토지에 인접한 타인 소유의 토지를 자신 소유 토지의 일부로 알고서 점유하게 된 자는 나중에 그 토지가 자신 소유의 토지가 아니라는 점을 알게 되었다거나 지적측량 결과 경계 침범 사실이 밝혀지고 그로 인해 상호분쟁이 있었다고 하더라도 그러한 사정만으로 그 점유가 타주점유로 전환되는 것은 아니다(대법원 2001. 5. 29. 선고 2001다5913 판결 등 참조).  위 법리에 비추어 보면, 원심이 1974. 10. 14.경 이 사건 (지번 1 생략) 대지에 관하여 경계측량이 이루어져 그 당시 판시 (다), (라)부분의 점유자인 소외 1이 그 부분이 원고 소유인 이 사건 (지번 1 생략) 대지의 경계를 침범한 사실을 확인하였다는 점만으로 소외 1의 판시 (다), (라)부분의 점유가 타주점유로 전환되었다고 판단한 것은 자주점유의 타주점유로의 전환에 관한 법리를 오해하여 판결에 영향을 미친 위법이 있다.   나. 시효이익의 포기는 시효의 완성으로 인하여 생기는 법률상의 이익을 받지 않겠다는 적극적이고 일방적인 의사표시로서, 취득시효 이익의 포기와 같은 상대방 있는 단독행위는 그 의사표시로 인하여 권리에 직접적인 영향을 받는 상대방에게 도달하는 때에 효력이 발생하고(대법원 2011. 7. 14. 선고 2011다23200 판결 등 참조), 시효이익 포기의 의사표시가 존재하는지의 판단은 표시된 행위나 의사표시의 내용과 동기 및 경위, 당사자가 의사표시 등에 의하여 달성하려고 하는 목적과 진정한 의도 등을 종합적으로 고찰하여 사회정의와 형평의 이념에 맞도록 논리와 경험의 법칙, 그리고 사회일반의 상식에 따라 객관적이고 합리적으로 이루어져야 한다(대법원 2013. 2. 28. 선고 2011다21556 판결 등 참조).   그런데 제1심증인 소외 8의 증언에 의하더라도 1992. 6. 15.경 이 사건 (지번 1 생략) 대지에 관하여 소외 6으로 보이는 휠체어를 탄 사람이 경계측량에 입회하여 자신의 건물이 원고 소유의 이 사건 (지번 1 생략) 대지의 경계를 침범하여 점유하고 있다는 사실이 밝혀지자 장래에 건물을 재건축할 때 다시 정리를 해주겠다고 하였을 뿐, 경계를 침범하고 있는 부분을 그 당시에 바로 원고에게 반환하겠다거나 자신이 점유하고 있는 부분의 소유권을 포기하겠다는 의사를 명시적으로 밝히지는 않았을 뿐만 아니라, 장래의 약속에 대해서도 이를 문서화하지는 않겠다고 거부하였다는 것이고, 기록에 의하면 판시 (나)부분과 이 사건 (지번 1 생략) 대지 중 원고가 점유하는 부분 사이의 경계선인 원심 별지 도면 표시 ㅈ, ㅊ, ㄴ¹의 각 점을 차례로 연결한 부분(이하 ‘이 사건 경계선 부분’이라 한다)은 이 사건 (지번 2 생략) 대지 상의 철거되기 전 구 건물의 안방 벽으로 사용되고 있었을 뿐만 아니라 이 사건 (지번 1 생략) 대지와의 경계로 사용되고 있었던 관계로 2002. 10. 21.경 구 건물을 철거하고 피고 1 명의로 새로운 건물을 건축할 때 그대로 남겨 두어 현재는 선상 길이 10.7m, 폭 0.2m, 높이 2.3m의 시멘트블럭조 담장으로 남아 있는 사실, 2002. 10. 21.경 구 건물을 철거하고 피고 1 명의로 새로운 건물을 건축한 후 이 사건 경계선 부분에 담장을 설치할 무렵까지도 원고가 판시 (나)부분에 대하여 어떠한 요구를 하였다고 볼 아무런 자료가 없을 뿐 아니라, 이 사건 (지번 2 생략) 대지에 피고 1 명의로 새로운 건물이 건축된 이후 판시 (나)부분은 공터로서 주차장 부지로 사용되고 있었을 뿐이었음에도 소외 6이 사망할 때까지 원고가 소외 6을 상대로 종래의 약속 등을 근거로 판시 (나)부분의 인도를 요구한 적은 없는 사실 등을 알 수 있다.  위와 같은 사실관계를 앞서 본 법리에 비추어 살펴보면, 비록 소외 6이 경계측량에 입회하여 장래에 건물을 재건축할 때 다시 정리를 해주겠다는 발언을 하였다 하더라도 그러한 발언은 이미 자신의 점유로 인한 취득시효 기간이 경과한 판시 (나)부분에 대한 시효이익을 그 당시에는 포기할 의사가 없고, 장래에 구 건물을 철거하고 재건축할 때 시효이익의 포기 여부를 고려해보겠다는 의사의 표시에 불과하다고 할 것이어서, 그러한 사정만으로 소외 6이 원고를 상대로 취득시효 이익을 포기하는 적극적인 의사표시를 하였다고 보기는 어렵다.  그럼에도 원심은 이와 달리 판시와 같은 사정만으로 소외 6이 판시 (나)부분에 대한 취득시효 이익을 포기하였다고 판단하였으므로, 이러한 원심판결에는 논리와 경험의 법칙을 위반하여 자유심증주의의 한계를 벗어나거나, 취득시효 이익의 포기에 관한 법리를 오해하여 판결에 영향을 미친 위법이 있다.   3. 결론  그러므로 원심판결을 파기하고, 사건을 다시 심리·판단하도록 원심법원으로 환송하기로 하여 관여 대법관의 일치된 의견으로 주문과 같이 판결한다.대법관 김신(재판장) 민일영 이인복(주심) 박보영  \n",
      "\n",
      "참조조문 \n",
      "[1] 민법 제197조 제1항, 제245조 제1항 / [2] 민법 제111조 제1항, 제184조, 제245조 제1항 \n",
      "\n",
      "판시사항 :\n",
      "[1] 자신의 토지에 인접한 타인 소유의 토지를 자신의 토지 일부로 알고서 점유를 시작한 자가 나중에 그 토지가 자신의 소유가 아니라는 점을 알게 되었다거나 지적측량 결과 경계 침범 사실이 밝혀져 상호분쟁이 있었다는 사정만으로 그 점유가 타주점유로 전환되는지 여부(소극)[2] 취득시효 완성에 따른 시효이익 포기의 상대방 및 시효이익 포기의 의사표시가 존재하는지 판단하는 기준\n",
      "\n",
      "\n",
      "\n",
      "==================================================================================================================\n",
      "==================================================================================================================\n",
      "사건 명 : 종회의원선거무효확인등 (유사도: 94.23%)\n",
      "사건 번호 : 2011다101155 \n",
      "사건 종류 : 민사\n",
      "판례 일련번호 : 179045 \n",
      "법원 : 대법원 \n",
      "\n",
      "판례 내용 :\n",
      "【원고, 상고인】 【피고, 피상고인】 대한불교원효종 외 1인 (소송대리인 법무법인 세종 담당변호사 변희찬 외 2인)【원심판결】 서울고법 2011. 10. 27. 선고 2010나96203 판결【주    문】  원심판결 중 피고 2에 대한 부분을 파기하고, 제1심판결 중 피고 2에 대한 부분을 취소하며, 피고 2에 대한 소를 모두 각하한다.  원심판결 중 피고 대한불교원효종에 대한 총무원장의 임명 무효 확인 청구 부분을 파기하고, 이 부분 사건을 서울고등법원에 환송한다.  피고 대한불교원효종에 대한 나머지 상고를 기각한다.  원고들과 피고 2 사이에 생긴 소송총비용은 원고들이 부담한다.【이    유】  1. 피고 2에 대한 소의 적법 여부에 관하여 직권으로 판단한다.법인 아닌 사단인 종교단체의 대표자 또는 구성원의 지위에 관한 확인소송에서 그 대표자 또는 구성원 개인을 상대로 제소하는 경우에는 그 청구를 인용하는 판결이 내려진다 하더라도 그 판결의 효력이 해당 단체에 미친다고 할 수 없기 때문에 대표자 또는 구성원의 지위를 둘러싼 당사자들 사이의 분쟁을 근본적으로 해결하는 가장 유효적절한 방법이 될 수 없으므로, 그 단체를 상대로 하지 않고 대표자 또는 구성원 개인을 상대로 한 청구는 확인의 이익이 없어 부적법하다(대법원 1991. 7. 12. 선고 91다12905 판결, 대법원 2011. 2. 10. 선고 2006다65774 판결 등 참조).  기록에 의하면, 원고들은 피고 대한불교원효종(이하 ‘피고 원효종’이라 한다)의 종도들로서 피고들을 상대로 피고 원효종이 2009. 4. 3. 당선확정한 종회의원의 선출과 피고 원효종이 2009. 4. 13.경 피고 2에 대하여 한 총무원장 임명에 대한 무효 확인을 구하는 소를 제기하였다가, 원심에서 피고 2에 대하여 위 피고가 피고 원효종의 대표자 지위에 있지 아니함을 확인하는 청구를 추가하였는데, 제1심은 원고들의 청구를 모두 기각하였고, 원심 역시 원고들의 항소를 기각하는 한편 원심에서 추가된 피고 2에 대한 위 청구를 기각하였음을 알 수 있다.  그러나 앞서 본 법리에 비추어 보면, 피고 원효종의 종회의원 선출 및 피고 2에 대한 총무원장 임명의 효력이나 그에 따른 피고 2의 대표자로서의 지위를 다투는 원고들로서는 피고 원효종을 상대로 하여 그 효력 유무나 지위의 존부에 관한 확인판결을 받음으로써 피고 원효종의 총무원장 임명 등을 둘러싼 법률적 불안이나 위험상태를 유효적절하게 제거할 수 있고, 피고 원효종이 아닌 피고 2 개인을 상대로 한 확인판결은 피고 원효종에 그 효력이 미치지 아니하여 당사자들 사이의 분쟁을 근본적으로 해결하는 가장 유효적절한 방법이 될 수 없으므로, 피고 2에 대하여 위 각 확인을 청구하는 소는 모두 즉시확정의 이익이 없다고 보아야 한다.  그렇다면 피고 2에 대한 소는 모두 그 확인의 이익이 없어 부적법하고, 이는 직권조사사항으로서 당사자의 주장 여부에 관계 없이 법원이 직권으로 판단하였어야 한다. 그럼에도 이와 달리 원심은, 제1심과 마찬가지로 이를 간과하고 본안에 관하여 나아가 심리·판단하고 말았다. 따라서 위와 같은 원심의 판단에는 확인의 소에서의 소송요건인 확인의 이익에 관한 법리를 오해하여 판결에 영향을 미친 위법이 있다.  2. 피고 원효종에 대한 상고이유(상고이유서 제출기간이 지난 후에 제출된 상고이유보충서 등 서면들의 기재는 상고이유를 보충하는 범위 내에서)를 판단한다.  가. 피고 원효종의 대표자 변경에 대하여  법인 아닌 사단의 대표자가 그 대표권을 잃은 때에는 소송절차가 중단되나, 대표자의 변경이 있다 하여도 소송대리인이 있는 경우에는 소송절차가 중단되지 아니한다(민사소송법 제64조, 제58조, 제235조, 제238조 등 참조).  기록에 의하면, 이 사건의 상고심 계속 중에 피고 원효종의 총무원장이 변경되었다는 내용의 자료가 제출되었으나, 피고 원효종의 대표자가 변경되었다 하더라도 그 소송대리인이 있는 이상 소송절차가 중단되지 아니하므로, 피고 원효종에 대한 상고이유에 나아가 판단하기로 한다.  나. 종회의원 선출 무효 확인에 관한 상고이유에 대하여(상고이유 제1, 2, 3점)  원심은 제1심판결 이유를 일부 인용하고 판시 이유를 추가하여, (1) 원고들 제출 증거만으로는 피고들이 이 사건 종회의원의 선출과정에서 종도 수를 제대로 파악하지 않았거나 신청서류를 허위로 기재하고 그것을 기초로 법원에 종회의원 선출허가 신청을 하여 법원으로부터 실제의 종도 수와 다른 교구별 종회의원 정수의 배정을 받았다고 인정하기에 부족하다고 판단하고, (2) 피고 원효종의 각 계파의 수장(소외 1, 소외 2, 소외 3 및 피고 2이다)이 모두 당사자 등으로 참여한 서울고등법원 2007라1580 임시종정 선임 비송사건의 항고심에서 임시종정으로 선임된 피고 2가 종도 수를 파악하기 위하여 법보신문 등 불교신문에 수회에 걸쳐 승려증 등을 갱신할 것을 요청하는 공지를 게재하였고, 그 무렵 피고 원효종의 인터넷 홈페이지에도 위와 같은 내용을 게재하였으며, 각 계파측 사람들이 참여한 회의를 개최하기도 하였고, 피고 2와 다른 각 계파의 수장인 소외 1, 소외 2, 소외 3에게 협조공문을 보내어 종도의 승적부를 제출하여 줄 것을 요청한 판시 사정 등에 비추어 볼 때, 이 사건 종회의원 선출과정에서 소외 1측 계파 및 소외 2측 계파의 종도들이 배제되었다고 보기 어렵다고 판단한 후, (3) 추천인 자격이 없는 자가 종회의원을 추천하거나 종도가 아닌 자가 교구 선거위원이 되었고 후보자들끼리 서로 추천하였으며 추천인이 여러 명을 추천하거나 추천인과 선거위원이 동일인이라는 등의 사유를 들어 종회의원 선출상의 절차적인 위법이 있다는 주장에 대하여, 피고 원효종 종회법(이하 ‘종회법’이라 한다)에 종회의원 후보자가 교차로 추천할 수 없다거나 추천인이 여러 명의 후보자를 추천할 수 없다는 등의 규정이 없고, 위와 같이 피고들이 피고 원효종의 오랜 분열로 인하여 종도 수를 파악하기 힘들자 여러 방법을 통하여 종도 수를 파악하려고 한 점 등에 비추어 보면 피고들이 법원에 종회의원 선출을 위하여 제출한 선거인 명부에 기재되지 아니한 종도가 있을 수 있고 종도가 변경될 수도 있다는 등의 판시와 같은 이유를 들어, 원고들의 주장 사정만으로는 이 사건 종회의원 선출이 무효로 될 정도의 절차적 위법이 있다고 단정하기 어렵다고 보고 원고들의 위 주장을 받아들이지 아니하였다.  판결서의 이유에는 주문이 정당하다는 것을 인정할 수 있을 정도로 당사자의 주장, 그 밖의 공격·방어방법에 관한 판단을 표시하면 되고 당사자의 모든 주장이나 공격·방어방법에 관하여 판단할 필요가 없다(민사소송법 제208조).  원심판결 및 원심이 인용한 제1심판결 이유를 적법하게 채택된 증거들에 비추어 살펴보면, 원심의 이유 설시에 일부 부족한 부분이 있지만, 위와 같은 원심의 판단에 논리와 경험의 법칙에 반하여 자유심증주의의 한계를 벗어나 필요한 심리를 다하지 아니하거나 관련 법리를 오해하고 판단을 누락하여 판결에 영향을 미친 위법이 없다.  다. 피고 원효종 총무원장의 임명 무효에 관한 상고이유 중 안건 통지 절차의 하자에 대하여(상고이유 제7점)  (1) 원심판결 및 원심이 인용한 제1심판결 이유와 적법하게 채택한 증거들에 의하면 다음의 사실을 알 수 있다.  (가) 피고 원효종은 종헌에 의하여 종도로 구성된 불교단체로서, 입법기관으로 종회를 두고 있고, 그 구성원인 종도들의 총회를 별도로 두고 있지 않다.  종회는 각 교구별로 종도 수에 따라 배정된 수만큼 각 교구의 선거위원회에서 선출된 종회의원으로 구성되고, 종헌 개정 및 종법 제정, 예산안 심의 및 결산 승인, 종정 추대 및 총무원장 등의 선출 등 피고 원효종의 중요한 의사를 결정하는 기능을 한다.  총무원장은 피고 원효종의 종무 전반을 총괄·집행하는 최고집행기관인 총무원의 책임자로서, 종회에서 선출하여 종정이 임명한다.  (나) 피고 원효종은 1989. 6. 9.경부터 종정 및 총무원장의 선임 등을 둘러싸고 내부 분쟁이 시작되어 4개의 계파로 나뉘어져 분쟁이 계속되어 왔다. 피고 2는 위 임시종정 선임 비송사건의 항고심에서 2008. 8. 25. 서울고등법원으로부터 임시종정으로 선임되었으나, 종정으로서의 통상적인 업무 이외에 종회의원의 선출, 종회의 구성 및 종정 선출을 위한 종회 소집에 관하여는 사전에 서울고등법원의 허가를 받도록 권한 행사가 제한되었다.  이에 따라 피고 2는 2008. 11. 7. 서울고등법원으로부터 피고 원효종의 종회를 구성하기 위하여 총무원장의 직무를 임시로 수행할 자로 소외 3을 임명하는 것에 대한 허가를 받고, 나아가 2009. 3. 4. 서울고등법원으로부터 피고 원효종의 종회의원을 선출하는 일정과 절차 및 2009. 4. 13.자의 종회 개최에 관한 허가를 받은 다음, 각 교구별로 종회의원 선출과정을 거쳐 2009. 4. 3. 종회의원 24명을 당선 확정하였다.  (다) 피고 원효종은 2009. 4. 13. 제1회 종회(이하 ‘이 사건 종회’라 한다)를 개최하였는데, 종회의원들에게 미리 그 회의의 목적사항이나 안건을 통지하지 않았다.  (라) 이 사건 종회에는 종회의원 24명 중 19명이 참석하였는데, 이 사건 종회에서 총무원장 선출 안건이 제기되자 일부 종회의원들은 사전에 회의 안건내용이 통지되지 아니하였음을 이유로 이의를 제기하였으나, 13명의 종회의원이 그 종회에서 총무원장을 선출하는 데 찬성하였고, 피고 2가 단독 후보로 추천되어 총무원장으로 선출되었으며, 이에 따라 임시종정인 피고 2가 자신을 총무원장으로 임명하였다.  (마) 종회법 제17조는 종회 개회 1개월 전에 의장이 종정의 동의를 얻어 소집하지만, 종회 의원선거 후 첫 종회는 종정이 소집하도록 정하고 있다. 그리고 피고 원효종의 종회법 등에는 구체적인 종회 소집통지 방법에 관한 규정을 두고 있지 않다.  (2) (가) 사단법인의 경우에 총회의 소집은 1주간 전에 그 회의의 목적사항을 기재한 통지를 보내거나 기타 정관에 정한 방법에 의하여야 하고(민법 제71조), 정관에 다른 규정이 없다면 총회는 위 절차에 따라 통지한 사항에 관하여서만 결의할 수 있다(제72조). 이는 사원이 결의를 할 목적사항을 사전에 알고서 회의 참석 여부나 결의사항에 대한 찬반의사를 미리 준비하게 하는 데에 그 취지가 있으므로, 회의의 목적사항은 사원이 안건이 무엇인지를 알 수 있도록 기재하여야 한다. 이와 같이 법인격을 전제로 하지 아니하는 민법 규정들은 원칙적으로 법인 아닌 사단의 경우에도 유추적용되므로(대법원 2006. 4. 20. 선고 2004다37775 전원합의체 판결), 법인 아닌 사단의 총회에서 회의 소집 통지에 목적 사항으로 기재하지 않은 사항에 관하여 결의한 때에는 구성원 전원이 회의에 참석하여 그 사항에 의하여 의결한 경우가 아닌 한 그 결의가 원칙적으로 무효라고 할 것이다(대법원 2013. 2. 14. 선고 2010다102403 판결 등 참조).  법인 아닌 사단에 해당하는 피고 원효종에서는 종도들로 구성된 총회를 별도로 두고 있지 않고 종회가 종회의원들이 각 교구의 종도들을 대표하여 피고 원효종의 중요한 의사를 결정하는 대의기관으로서 사단에서의 총회와 같은 기능을 담당하고 있다. 따라서 피고 원효종의 종회에 대하여도 법인 아닌 사단의 총회에 관한 절차 및 법리는 원칙적으로 적용 가능할 것으로 보인다.  (나) 종회법 제17조는 종회 개회 1개월 전에 의장이 종정의 동의를 얻어 소집하도록 되어 있는데, 위와 같이 1개월 전의 소집 기간을 둔 것은 종회의원에게 종회의 목적사항 등을 알리고 종회의원으로 하여금 그 참석 여부 및 그 목적사항인 안건에 대한 찬반의사 등의 의견을 결정할 수 있는 기간을 주려는 것이라 보인다. 특히 총무원장 등의 선출의 경우에는 종정이나 종회의원 및 종도들이 적절한 후보자를 추천하고 그 후보자에 대한 검증 또는 의견 교환 등의 사전절차를 거쳐 종회에서 의결할 수 있도록 함이 타당할 것이므로 이를 위하여서도 상당한 기간을 두고 그 소집 목적사항을 알릴 필요가 있다. 따라서 종회법 규정에는 실질적으로 종회 소집의 목적사항을 사전에 통지하려는 취지가 담겨져 있다고 할 것이다.  이 사건 종회가 비록 종회 의원선거 후의 첫 종회로서 종회법에 따라 의장이 아닌 임시종정이 소집할 수 있지만, 이는 종회 의장이 선출되지 아니한 상태에서의 소집권자를 정한 것뿐이므로, 그러한 사정만으로 종회 개회 1개월 전에 소집 절차를 취하도록 한 종회법 규정의 취지를 무시할 것은 아니다. 한편 서울고등법원의 임시종정 선임 결정에서는 종회의원의 선출 외에도 종회의 구성 및 종정의 선출을 위한 종회 소집에 관하여 사전에 임시종정이 서울고등법원의 허가를 받도록 권한 행사를 제한하고 있는데, 그 후 이루어진 서울고등법원의 종회의원 선출 허가에서 종회의원 선출과 함께 이 사건 종회 개최가 허가되었지만 그 소집목적이 무엇인지는 위 허가에 나타나 있지 않으므로, 법원의 소집허가를 받은 사정만을 가지고 이 사건 종회에서 총무원장을 선출하는 것까지 예정되어 통지되었다고 보기도 어렵다.  (다) 그리고 총무원장은 피고 원효종의 종무 전반을 총괄·집행하는 총무원의 책임자이므로, 총무원장의 선출은 단지 일상적인 운영을 위하여 필요한 사항이 아니라 피고 원효종의 종무 및 운영 전반에 매우 큰 영향을 미치는 최고집행기관의 책임자를 선출하는 것으로서 종회의 중요한 기능에 해당한다.  따라서 총무원장의 선출을 위한 종회의 경우에는 종회 절차를 준수하여 공정성을 기하여야 할 것이다. 특히 피고 원효종은 20년에 가까운 장기간의 분쟁을 종료시키기 위해서 법원으로부터 임시종정의 선임, 종회의원 선출 허가를 받는 등의 엄격한 절차를 거쳐서 정상화를 진행하여 왔으므로, 피고 원효종의 정상화를 위해서 반드시 거쳐야 하는 이 사건 총무원장 선출에 관하여는 그 종회 의결 절차를 준수할 필요성이 더욱 크다.  (3) 이러한 사정들에 비추어 위 사실관계를 살펴보면, (가) 이 사건 종회에서 총무원장을 선출하기 위하여는 사전에 1개월 이상의 상당한 기간을 두고 종회의원들에게 그 선출에 관한 회의 목적사항을 알려 이 사건 종회에의 참석 여부나 후보자의 추천 및 그 찬반의사 등에 관한 의견을 미리 준비하게 할 필요가 있었다고 할 것이고, (나) 그런데 이 사건 종회는 총무원장을 선출한다는 목적사항을 미리 통지하지 아니한 채 소집되었고 안건 결의에 대한 이의가 있었음에도 종회의원들 중 일부만 참석한 상태에서 피고 2를 총무원장으로 선출하는 결의를 하였으므로, 그 선출 결의는 종회의 소집 절차 내지 선출 절차가 법인 아닌 사단에서의 총회 절차에 관한 일반 법리 및 종회법 제17조의 취지에 어긋나고 현저하게 불공정하여 무효이며, 그 선출 결의에 터잡아 이루어진 총무원장 임명도 무효라고 할 것이다.  (4) 그럼에도 이와 달리 원심은 제1심판결을 인용하여, 종회에서 총무원장을 선출하기 위하여 사전에 안건이 고지되어야 함을 인정할 증거가 없다는 이유만으로, 이 사건 종회에서 총무원장으로 피고 2를 선출한 결의 및 그 결의에 기초한 총무원장 임명이 무효라는 원고들의 주장을 받아들이지 아니하였다.  따라서 이러한 원심의 판단에는 종회의 소집통지 절차에 관한 법리를 오해하여 판결에 영향을 미친 위법이 있다. 이를 지적하는 원고들의 상고이유 주장은 이유 있다.  3. 그러므로 (1) 원심판결 중 피고 2에 대한 부분을 파기하되, 이 부분은 이 법원이 직접 재판하기에 충분하므로 자판하기로 하여 제1심판결 중 위 피고에 대한 부분을 취소하며, 위 피고에 대한 소를 모두 각하하고, (2) 원심판결 중 피고 원효종에 대한 총무원장 임명 무효 확인 청구 부분에 관하여 원고들의 나머지 상고이유에 대한 판단을 생략하고 이 부분을 파기하여, 이 부분 사건을 다시 심리·판단하게 하기 위하여 원심법원에 환송하며, 피고 원효종에 대한 나머지 상고를 기각하고, (3) 원고들과 피고 2 사이에 생긴 소송총비용은 패소자들이 부담하기로 하여, 관여 대법관의 일치된 의견으로 주문과 같이 판결한다.대법관 김소영(재판장) 이인복 김용덕(주심) 고영한  \n",
      "\n",
      "참조조문 \n",
      "[1] 민사소송법 제250조 / [2] 민법 제31조, 제71조, 제72조 \n",
      "\n",
      "판시사항 :\n",
      "[1] 법인 아닌 사단인 종교단체의 대표자 또는 구성원의 지위에 관한 확인소송을 단체가 아닌 대표자 또는 구성원 개인을 상대로 제기할 확인의 이익이 있는지 여부(소극)[2] 법인 아닌 사단의 총회에서 회의 소집 통지에 목적 사항으로 기재하지 않은 사항에 관하여 결의한 경우, 결의의 효력(원칙적 무효)\n",
      "\n",
      "\n",
      "\n",
      "==================================================================================================================\n",
      "==================================================================================================================\n",
      "사건 명 : 거절결정(상) (유사도: 94.22%)\n",
      "사건 번호 : 2015후1348 \n",
      "사건 종류 : 특허\n",
      "판례 일련번호 : 183020 \n",
      "법원 : 대법원 \n",
      "\n",
      "판례 내용 :\n",
      "【원고, 피상고인】 삼성물산 주식회사 (소송대리인 특허법인 수 담당변리사 서수진)【피고, 상고인】 특허청장【원심판결】 특허법원 2015. 7. 17. 선고 2015허1874 판결【주    문】  원심판결을 파기하고, 사건을 특허법원에 환송한다.【이    유】  상고이유를 판단한다.  1. 상표의 유사 여부는 대비되는 상표를 외관, 호칭, 관념의 세 측면에서 객관적, 전체적, 이격적으로 관찰하여 거래상 오인·혼동의 염려가 있는지에 따라 판단하여야 한다. 특히 도형상표들에 있어서는 그 외관이 주는 지배적 인상이 동일·유사하여 두 상표를 동일·유사한 상품에 다 같이 사용하는 경우 일반 수요자에게 상품의 출처에 관하여 오인·혼동을 일으킬 염려가 있다면 두 상표는 유사하다고 보아야 한다. 또한 상표의 유사 여부의 판단은 두 개의 상표 자체를 나란히 놓고 대비하는 것이 아니라 때와 장소를 달리하여 두 개의 상표를 대하는 일반 수요자에게 상품 출처에 관하여 오인·혼동을 일으킬 우려가 있는지의 관점에서 이루어져야 하고, 두 개의 상표가 그 외관, 호칭, 관념 등에 의하여 일반 수요자에게 주는 인상, 기억, 연상 등을 전체적으로 종합할 때 상품의 출처에 관하여 오인·혼동을 일으킬 우려가 있는 경우에는 두 개의 상표는 서로 유사하다고 보아야 한다(대법원 2013. 3. 14. 선고 2010도15512 판결 등 참조).이 사건 출원상표이 사건 선등록상표  2. 이러한 법리에 비추어 ‘가방’ 등을 지정상품으로 하고 오른쪽 위와 같이 구성된 이 사건 출원상표(상표출원번호 생략)와 ‘서류가방’ 등을 지정상품으로 하고 오른쪽 아래와 같이 구성된 이 사건 선등록상표가 유사한지 살펴본다.  일반 수요자의 직관적 인식을 기준으로 두 상표의 외관을 이격적으로 관찰하면, 두 표장은 모두 검은색 도형 내부에 옆으로 누운 아치형의 도형 2개가 상하로 배치되어 있는 점, 검은색 도형의 왼쪽 부분이 오른쪽 부분보다 2배 정도 두꺼운 점 등에서 공통되고, 알파벳 ‘B’를 이용하여 도안화한 것으로 보이는 점에서 모티브가 동일하여 전체적인 구성과 거기에서 주는 지배적 인상이 유사하다.  다만 이 사건 출원상표는 검은색 도형이 오각형이어서 상부가 뾰족한 형상을 이루는 반면 이 사건 선등록상표는 검은색 도형이 사각형이어서 상부가 평평한 형상인 점, 이 사건 출원상표는 검은색 도형 내부에 있는 2개의 아치형 도형의 크기 차이가 있음이 비교적 분명히 드러나는 반면 이 사건 선등록상표는 2개의 아치형 도형의 크기가 거의 같은 점 등에서 차이가 있으나, 이는 이격적 관찰로는 쉽게 파악하기 어려운 정도의 차이에 불과하다고 보인다.  이와 같이 두 표장은 그 외관이 주는 지배적인 인상이 유사하여 동일·유사한 상품에 다 같이 사용하는 경우 일반 수요자에게 그 출처에 관하여 오인·혼동을 일으킬 염려가 있으므로 서로 유사하다고 할 것이다.  그럼에도 원심은 이 사건 출원상표와 이 사건 선등록상표는 그 지배적인 외관이 확연히 다르다는 이유 등으로 서로 유사하다고 보기 어렵다고 판단하였으니, 원심판결에는 상표의 유사 여부 판단에 관한 법리를 오해하여 판결에 영향을 미친 위법이 있다. 이 점을 지적하는 상고이유의 주장은 이유 있다.  3. 그러므로 원심판결을 파기하고, 사건을 다시 심리·판단하게 하기 위하여 원심법원에 환송하기로 하여 관여 대법관의 일치된 의견으로 주문과 같이 판결한다.대법관 조희대(재판장) 이상훈(주심) 김창석 박상옥  \n",
      "\n",
      "참조조문 \n",
      "[1] 상표법 제7조 제1항 제7호, 제12호 / [2] 상표법 제7조 제1항 제7호, 제12호 \n",
      "\n",
      "판시사항 :\n",
      "[1] 상표가 유사한지 판단하는 기준 및 도형상표에서 상표가 유사한지 판단하는 기준 / 상표가 유사한지 판단하는 방법[2] 특허청 심사관이 가방 등을 지정상품으로 하는 甲 주식회사의 출원상표 “”에 대하여 서류가방 등을 지정상품으로 하는 선등록상표 “”와 표장 및 지정상품이 유사하다는 등의 이유로 거절결정을 하자, 甲 회사가 특허심판원에 불복심판을 청구하였으나 특허심판원이 기각하는 심결을 한 사안에서, 두 표장은 외관이 주는 지배적인 인상이 유사하여 동일·유사한 상품에 다 같이 사용하는 경우 일반 수요자에게 출처에 관하여 오인·혼동을 일으킬 염려가 있으므로 서로 유사하다고 한 사례\n",
      "\n",
      "\n",
      "\n",
      "==================================================================================================================\n",
      "==================================================================================================================\n",
      "사건 명 : 공유물분할 (유사도: 94.13%)\n",
      "사건 번호 : 2011다69190 \n",
      "사건 종류 : 민사\n",
      "판례 일련번호 : 171221 \n",
      "법원 : 대법원 \n",
      "\n",
      "판례 내용 :\n",
      "【원고, 상고인】 【피고, 피상고인】 【피    고】 【원심판결】 서울고법 2011. 7. 20. 선고 2010나99561 판결【주    문】  원심판결을 파기하고, 사건을 서울고등법원에 환송한다.【이    유】  상고이유를 살펴본다.  1. 원심의 판단  (1) 원심판결 이유에 의하면, 원고와 피고들이 공유하는 인천 부평구 산곡동 (지번 1 생략) 대 147.3㎡ 및 같은 동 (지번 2 생략) 대 398.3㎡ 양 지상(이하 양 대지를 합하여 ‘이 사건 대지’라 한다)에 있는 미등기 무허가 건물(이하 ‘이 사건 건물’이라 한다)의 분할을 구하는 이 사건 공유물분할의 소에 관하여, 원심은 그 채용 증거들을 종합하여, ① 이 사건 대지와 건물은 원래 원·피고들의 어머니인 소외인 소유였는데 소외인이 1990. 4. 20. 이 사건 대지 부분만 원고에게 증여하였고, 소외인이 2002. 4. 27. 사망하자 원·피고들이 이 사건 건물의 소유권을 각 1/7씩 상속하였으며, 피고들은 이 사건 건물에 관하여 관습법상 법정지상권을 취득한 사실, ② 피고 2, 피고 4, 피고 5는 피고 1에게 이 사건 건물의 관리를 위임하여 그동안 피고 1이 그들을 대신해 이 사건 건물에 관하여 임대차계약을 체결하거나 이 사건 건물을 수선하여 온 사실, ③ 이 사건 건물은 대부분 점포로 이용 중이고, 잔존내용연수는 22년이며, 이 사건 건물에 관하여 체결된 임대차계약상 연간 차임 합계는 6,480만 원이고, 원심 변론종결일에 가까운 2010. 4. 26. 무렵 이 사건 건물의 시가는 110,822,300원이나, 이 사건 건물의 위치에 따라 차임 및 시가는 많은 차이가 있는 사실을 인정하였다.  (2) 원심은 위와 같은 사실인정을 기초로, ① 이 사건 건물을 현물분할하는 것은 사실상 불가능하고 건물의 경제적 가치를 크게 훼손할 뿐만 아니라 당사자들도 원하지 않고 있어 적절하지 않은 점, ② 이 사건 대지가 원고의 단독 소유라고는 하나 원고는 애초에 이 사건 건물이 건축된 상태에서 대지를 취득하였을 뿐만 아니라 피고들에게는 관습법상 법정지상권을 설정해 줄 의무를 부담하고 있으므로 이 사건 대지를 사용·수익함에 있어서 그와 같은 제한된 상태를 수인할 의무가 있는 점, ③ 이 사건 건물을 원고 단독 소유로 한다면 피고들이 이 사건 건물의 지상권 존속기간 동안 누릴 수 있는 현재와 같은 차임 상당의 이익을 박탈하는 것이 되므로 공유물분할 결과 당사자들 사이에 그 이익의 불균형이 커지는 점, ④ 통상 건물과 대지의 소유자를 일치시키는 것이 각 부동산의 활용도와 경제적 가치를 극대화할 수 있다고는 하나, 이는 건물의 존속을 전제로 할 때 타당한 것이지 이 사건과 같이 원고가 이 사건 건물의 노후화 등을 이유로 이 사건 건물을 철거할 의사를 밝히고 있음을 고려하면 이 사건 대지의 소유자가 원고라고 하더라도 이 사건 건물을 원고 단독 소유로 하는 것이 꼭 이 사건 건물의 경제적 가치를 극대화하는 것은 아닌 점 등을 종합하여 볼 때 이 사건 건물에 대한 공유물분할방법은 현물분할이 아닌 경매를 통한 분할방법에 의하여야 할 것이므로, 이 사건 건물을 경매에 부쳐 그 대금에서 경매비용을 공제한 나머지 금액을 공유자들인 원고와 피고들에게 그 지분 비율에 따라 각 1/7씩 분배함이 상당하다고 판단하였다.  2. 대법원의 판단  그러나 원심의 위와 같은 판단은 다음과 같은 이유에서 수긍할 수 없다.  가. 민사집행법 제81조 제1항 제2호 단서는 등기되지 아니한 건물에 대한 강제경매신청서에는 그 건물에 관한 건축허가 또는 건축신고를 증명할 서류를 첨부하여야 한다고 규정함으로써 적법하게 건축허가나 건축신고를 마친 건물이 사용승인을 받지 못한 경우에 한하여 부동산 집행을 위한 보존등기를 할 수 있게 하였고, 같은 법 제274조 제1항은 공유물분할을 위한 경매와 같은 형식적 경매는 담보권 실행을 위한 경매의 예에 따라 실시한다고 규정하며, 같은 법 제268조는 부동산을 목적으로 하는 담보권 실행을 위한 경매절차에는 같은 법 제79조 내지 제162조의 규정을 준용한다고 규정하고 있으므로, 건축허가나 신고 없이 건축된 미등기 건물에 대하여는 경매에 의한 공유물분할이 허용되지 않는다.  나. 원심의 채용 증거들과 기록에 의하면 이 사건 건물은 등기되지 않았을 뿐 아니라 건축허가나 신고 없이 지어진 사실을 알 수 있는바, 원심이 이 사건 건물에 대한 공유물분할을 경매에 의하여야 한다고 판단한 것은 부동산 경매의 대상에 관한 법리를 오해하여 판단을 그르친 것이다.  한편 공유관계의 발생원인과 공유지분의 비율 및 분할된 경우의 경제적 가치, 분할 방법에 관한 공유자의 희망 등의 사정을 종합적으로 고려할 때 당해 공유물을 특정 공유 \n",
      "\n",
      "참조조문 \n",
      "민법 제269조, 민사집행법 제81조 제1항 제2호, 제268조, 제274조 제1항 \n",
      "\n",
      "판시사항 :\n",
      "건축허가나 신고 없이 건축된 미등기 건물에 대하여 경매에 의한 공유물분할이 허용되는지 여부(소극)\n",
      "\n",
      "\n",
      "\n",
      "==================================================================================================================\n",
      "==================================================================================================================\n",
      "사건 명 : 명의신탁해지등을원인으로한소유권이전등기 (유사도: 94.12%)\n",
      "사건 번호 : 2018다299099 \n",
      "사건 종류 : 민사\n",
      "판례 일련번호 : 225331 \n",
      "법원 : 대법원 \n",
      "\n",
      "판례 내용 :\n",
      "【원고, 상고인】 원고 (소송대리인 변호사 이재호)【피고(선정당사자), 피상고인】 피고(선정당사자)【원심판결】 대전지법 2018. 11. 28. 선고 2018나102063 판결【주    문】  원심판결을 파기하고, 사건을 대전지방법원에 환송한다.【이    유】  상고이유를 판단한다.   1. 부동산의 점유권원의 성질이 분명하지 않을 때에는 민법 제197조 제1항에 의하여 점유자는 소유의 의사로 점유한 것으로 추정되고, 점유자가 스스로 매매 또는 증여와 같이 자주점유의 권원을 주장하였으나 이것이 인정되지 않는 경우에도 그러한 사유만으로 자주점유의 추정이 번복된다거나 그 점유가 타주점유로 되는 것이 아니다. 또한 점유자의 점유가 소유의 의사 있는 자주점유인지 아니면 소유의 의사 없는 타주점유인지의 여부는 점유 취득의 원인이 된 권원의 성질이나 점유와 관계있는 모든 사정에 의하여 외형적·객관적으로 결정되어야 하기 때문에 점유자가 성질상 소유의 의사가 없는 것으로 보이는 권원에 바탕을 두고 점유를 취득한 사실이 증명되었거나, 점유자가 타인의 소유권을 배제하여 자기의 소유물처럼 배타적 지배를 하려는 의사를 가지고 점유하는 것으로 볼 수 없는 객관적 사정, 즉 점유자가 진정한 소유자라면 통상 취하지 아니할 태도를 나타내거나 소유자라면 당연히 취했을 것으로 보이는 행동을 취하지 아니한 경우 등 외형적·객관적으로 보아 점유자가 타인의 소유권을 배척하고 점유할 의사를 갖고 있지 아니하였던 것이라고 볼 만한 사정이 증명된 경우에 한하여 그 추정은 깨어지는 것이다(대법원 2000. 3. 16. 선고 97다37661 전원합의체 판결, 대법원 2002. 2. 26. 선고 99다72743 판결, 대법원 2014. 8. 26. 선고 2013다9888 판결 등 참조). 한편 공유토지는 공유자 1인이 그 전부를 점유하고 있다고 하여도 다른 특별한 사정이 없다면 그 권원의 성질상 다른 공유자의 지분비율의 범위 내에서는 타주점유라고 볼 수밖에 없지만, 공유자들이 분할 전 토지의 전체면적 중 각 점유 부분을 구분소유하게 된다고 믿고서 그 각 점유 부분의 대략적인 면적에 해당하는 만큼의 지분에 관하여 소유권이전등기를 경료받은 경우에는, 등기부상 공유자들이 각 토지의 일부 공유자로 되어 있다고 하더라도 그들의 점유가 권원의 성질상 타주점유라고 할 수는 없다(대법원 1996. 3. 22. 선고 95다53768 판결, 대법원 2007. 3. 29. 선고 2006다79995 판결 등 참조).  2. 원심은, 천안시 서북구 ○○읍○○리△△△-□□ 과수원 849㎡(이하 ‘이 사건 토지’라 한다) 중 원심 별지 도면 표시 선내 ‘ㄴ’ 부분 360㎡(이하 ‘이 사건 주택부지’라 한다)를 1987년경부터 20년 이상 점유하여 점유취득시효가 완성되었다는 원고의 주장에 대하여, 원고가 1989. 2. 22. 이 사건 토지 중 364/2651 지분에 관하여 소유권이전등기를 경료한 사실 등을 인정하면서도, 이 사건 각 토지에 관한 원고, 피고(선정당사자, 이하 ‘피고’라고만 한다)와 선정자 2의 매매 및 경매절차에서 해당 지분이 이 사건 각 토지에 대한 구분소유적 공유관계를 표상하는 것으로 취급된 것이라는 점을 인정하기 어려우므로 이 사건 토지에 관한 구분소유적 공유관계가 인정되지 않는다고 보고, 원고가 이 사건 주택부지를 점유하고 있다고 하더라도 다른 특별한 사정이 없는 한 권원의 성질상 다른 공유자의 지분비율의 범위 내에서는 타주점유라고 판단하고, 원고의 점유취득시효 주장을 배척하였다.   3. 그러나 원심의 판단은 아래와 같은 이유에서 받아들이기 어렵다.  가. 원심판결 이유와 기록에 의하면 다음과 같은 사실을 알 수 있다.  1) 이 사건 토지는 분할 전 충남 ◇◇군○○읍☆☆리△△△ 과수원 2,651㎡(이하 ‘이 사건 분할 전 토지’라 한다)의 일부였다가 1992. 12. 8. 충남 ▽▽군○○읍☆☆리△△△ 과수원 1,651㎡, 같은 리 △△△-◎ 과수원 975㎡와 같은 리 △△△-◁ 과수원 25㎡로 각 분할되었고, 위 같은 리 △△△ 과수원 1,651㎡는 최종적으로 같은 리 △△△ 과수원 802㎡와 이 사건 토지로 분할되었다.   2) 소외 1은 1987. 5.경 소외 2 명의로 이 사건 분할 전 토지 2,651㎡ 중 661㎡를 매수한 뒤 그중 364㎡(이 사건 주택부지와 같은 부분으로 보인다) 지상에 이 사건 주택을 건축하기로 하여 같은 달 28일 소외 2 명의로 건축허가를 받아 신축공사에 착수하였으나 자금사정으로 공사를 중단하였고, 소외 2는 1988. 2.경 이 사건 주택부지와 당시까지 축조된 건물을 일괄하여 소외 3에게 2,800만 원에 양도하였으며, 소외 3은 건축공사를 진행하지 않고 있다가 1989. 2. 20. 원고에게 이 사건 주택부지와 지상건 \n",
      "\n",
      "참조조문 \n",
      " [1] 민법 제197조 제1항 / [2] 민법 제197조 제1항, 제245조 제1항, 제262조 \n",
      "\n",
      "판시사항 :\n",
      "  [1] 부동산 점유권원의 성질이 분명하지 않은 경우, 민법 제197조 제1항에 따라 자주점유로 추정되는지 여부(적극) / 점유자가 스스로 자주점유의 권원을 주장하였으나 이것이 인정되지 않는다는 사정만으로 자주점유의 추정이 번복되거나 타주점유로 되는지 여부(소극) / 자주점유의 추정이 번복되는 경우  [2] 등기부상 공유자들이 공유토지 중 각 특정 부분을 구분소유하게 된다고 믿고서 각 점유 부분의 대략적인 면적에 해당하는 만큼의 지분에 관하여 소유권이전등기를 마친 경우, 각 점유가 권원의 성질상 타주점유인지 여부(소극)\n",
      "\n",
      "\n",
      "\n",
      "==================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 사용자 질의 입력\n",
    "query = input(\"상황을 작성해 주세요 (자세히 작성 시 검색에 도움이 됩니다.) :\\n\")\n",
    "recommended_courts = recommend_court(query)\n",
    "\n",
    "if isinstance(recommended_courts, str):\n",
    "    print(recommended_courts)\n",
    "else:\n",
    "    court_detail_OX = input(\"판례 및 판시 사항을 보시겠습니까? (Y/N): \\n\")\n",
    "    for event_Title, event_num, event_type, jPrecedent_num, court_name, court_detail, court_reference, court_Decision, similarity in recommended_courts:\n",
    "        print('==================================================================================================================')\n",
    "        if (court_detail_OX == 'N') or (court_detail_OX == 'n'):\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\n판례 일련번호 : {jPrecedent_num} \\n법원 : {court_name} \\n\\n참조조문 \\n{court_reference}\\n\\n\\n\")\n",
    "        else:\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\n판례 일련번호 : {jPrecedent_num} \\n법원 : {court_name} \\n\\n판례 내용 :\\n{court_detail} \\n\\n참조조문 \\n{court_reference} \\n\\n판시사항 :\\n{court_Decision}\\n\\n\\n\")\n",
    "        print('==================================================================================================================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be55d0b",
   "metadata": {},
   "source": [
    "# RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2866e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub\n",
    "#huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19868dd7",
   "metadata": {},
   "source": [
    "# 데이터 전처리 후 TF-IDF, cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8ea3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "def preprocess_text(data):\n",
    "    special_appellant = []\n",
    "    original_decision = []\n",
    "    order = []\n",
    "    reason = []\n",
    "    \n",
    "    for case in data:\n",
    "        try:\n",
    "            special_appellant.append(re.search(r'【특별항고인】\\s*(.*?)\\s*【', case, re.DOTALL).group(1).strip())\n",
    "        except AttributeError:\n",
    "            special_appellant.append('')\n",
    "\n",
    "        try:\n",
    "            original_decision.append(re.search(r'【원심결정】\\s*(.*?)\\s*【', case, re.DOTALL).group(1).strip())\n",
    "        except AttributeError:\n",
    "            original_decision.append('')\n",
    "\n",
    "        try:\n",
    "            order.append(re.search(r'【주\\s*문】\\s*(.*?)\\s*【', case, re.DOTALL).group(1).strip())\n",
    "        except AttributeError:\n",
    "            order.append('')\n",
    "\n",
    "        try:\n",
    "            reason.append(re.search(r'【이\\s*유】\\s*(.*)', case, re.DOTALL).group(1).strip())\n",
    "        except AttributeError:\n",
    "            reason.append('')\n",
    "\n",
    "    return {\n",
    "        '특별항고인': special_appellant,\n",
    "        '원심결정': original_decision,\n",
    "        '주문': order,\n",
    "        '이유': reason\n",
    "    }\n",
    "\n",
    "# Excel 파일에서 데이터 읽기\n",
    "df_law = pd.read_excel('data/law_recomended.xlsx')\n",
    "df_law_detail = df_law['court_detail'].tolist()\n",
    "\n",
    "# 판례 데이터 전처리\n",
    "case_data = preprocess_text(df_law_detail)\n",
    "\n",
    "# 판례 데이터를 데이터프레임으로 변환\n",
    "df = pd.DataFrame(case_data)\n",
    "\n",
    "# Komoran 형태소 분석기를 이용한 토큰화\n",
    "km = Komoran()\n",
    "\n",
    "# 형태소 분석을 문자열로 변환\n",
    "df['reason_token'] = df['이유'].apply(lambda x: ' '.join(km.morphs(x)))\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['reason_token'])\n",
    "\n",
    "# 특정 판례와 유사한 판례 출력\n",
    "def find_similar_cases(case, top_n=5):\n",
    "    # 입력된 상황을 TF-IDF로 변환\n",
    "    case_tfidf = tfidf_vectorizer.transform([case])\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    cosine_sim = cosine_similarity(case_tfidf, tfidf_matrix)\n",
    "    \n",
    "    # 유사도 계산 결과를 데이터프레임으로 변환\n",
    "    similarity_df = pd.DataFrame(cosine_sim, columns=df.index)\n",
    "    \n",
    "    # 가장 유사한 판례 인덱스 찾기\n",
    "    similar_cases = similarity_df.loc[0].sort_values(ascending=False).head(top_n)\n",
    "    return df.loc[similar_cases.index]\n",
    "\n",
    "# 상황 입력\n",
    "case_input = str(input(\"상황을 입력하세요: \"))\n",
    "\n",
    "# 입력된 상황과 유사한 판례 찾기\n",
    "similar_cases = find_similar_cases(case_input)\n",
    "\n",
    "print(similar_cases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04de9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('data/similarty.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 판례와 유사한 판례 출력\n",
    "def find_similar_cases(case, top_n=5):\n",
    "    # 입력된 상황을 TF-IDF로 변환\n",
    "    case_tfidf = tfidf_vectorizer.transform([case])\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    cosine_sim = cosine_similarity(case_tfidf, tfidf_matrix)\n",
    "    \n",
    "    # 유사도 계산 결과를 데이터프레임으로 변환\n",
    "    similarity_df = pd.DataFrame(cosine_sim, columns=df.index)\n",
    "    \n",
    "    # 가장 유사한 판례 인덱스 찾기\n",
    "    similar_cases = similarity_df.loc[0].sort_values(ascending=False).head(top_n)\n",
    "    return [df.loc[similar_cases.index], similarity_df]\n",
    "\n",
    "# 상황 입력\n",
    "case_input = str(input(\"상황을 입력하세요: \"))\n",
    "\n",
    "# 입력된 상황과 유사한 판례 찾기\n",
    "similar_cases = find_similar_cases(case_input)\n",
    "\n",
    "print(similar_cases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f44d5",
   "metadata": {},
   "source": [
    "## hugging hub pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install transformers[sentencepiece]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee964e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd106ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "input_dict = tokenizer.prepare_seq2seq_batch(\"who holds the record in 100m freestyle\", return_tensors=\"pt\") \n",
    "\n",
    "generated = model.generate(input_ids=input_dict[\"input_ids\"]) \n",
    "print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf93145",
   "metadata": {},
   "source": [
    "# DistilBERT BASE QA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a63b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed582f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textrankr\n",
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645055fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class OktTokenizer:\n",
    "    okt: Okt = Okt()\n",
    "\n",
    "    def __call__(self, text: str) -> List[str]:\n",
    "        tokens: List[str] = self.okt.phrases(text)\n",
    "        return tokens\n",
    "\n",
    "    \n",
    "from typing import List\n",
    "from textrankr import TextRank\n",
    "\n",
    "# 파일 경로\n",
    "file_path = 'data/context.xlsx'\n",
    "\n",
    "# 파일 로드\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[:4]\n",
    "df = df[['event_Title','court_detail']]\n",
    "\n",
    "text = df['court_detail']\n",
    "\n",
    "mytokenizer: OktTokenizer = OktTokenizer()\n",
    "textrank: TextRank = TextRank(mytokenizer)\n",
    "\n",
    "k: int = 3  # num sentences in the resulting summary\n",
    "\n",
    "# summarized: str = textrank.summarize(text, k)\n",
    "# print(summarized)  # gives you some text\n",
    "\n",
    "# if verbose = False, it returns a list\n",
    "summaries: List[str] = textrank.summarize(text, k, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# 파일 경로\n",
    "file_path = 'data/context.xlsx'\n",
    "\n",
    "# 파일 로드\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[:4]\n",
    "df = df[['event_Title','court_detail']]\n",
    "\n",
    "# 전처리 - 불용어 제거, 형태소 분석 등\n",
    "texts = [[word for word in doc.split()] for doc in df['court_detail']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda_model = LdaMulticore(corpus=corpus, \n",
    "                        id2word=dictionary,\n",
    "                        num_topics=10)\n",
    "\n",
    "# 주제 추출\n",
    "topics = lda_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# 시각화\n",
    "pyLDAvis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(pyLDAvis_data, 'lda_visualization.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "file_path = 'data/law_recomended.xlsx'\n",
    "\n",
    "# 파일 로드\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[:4]\n",
    "df = df[['event_Title','court_detail']]\n",
    "df['court_detail'] = df['court_detail'].str.extract(r'【이    유】(.*)', expand=False)\n",
    "df['court_detail'] = df['court_detail'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x) if pd.notnull(x) else x)\n",
    "df['court_detail'] = df['court_detail'].replace(['.', ',', '/'], '', regex=True)\n",
    "df['court_detail'] = df['court_detail'].str.strip()\n",
    "df.to_excel('context.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ca0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 파일 경로\n",
    "file_path = 'data/context.xlsx'\n",
    "\n",
    "# 파일 로드\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[:4]\n",
    "df = df[['event_Title','court_detail']]\n",
    "\n",
    "# BART 요약 모델과 토크나이저 불러오기\n",
    "summarizer_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# KoELECTRA QA 모델과 토크나이저 불러오기\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "\n",
    "# 각 판례의 내용을 기반으로 질문 생성\n",
    "def generate_question(row):\n",
    "    case_title = row.get('event_Title', '이 사건')\n",
    "    case_detail = row.get('court_detail', '')\n",
    "\n",
    "    patterns = {\n",
    "        '저작권': [\n",
    "            f\"{case_title}에서 저작권법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 저작권법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 저작권 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 저작권 침해에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 저작권법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '특허': [\n",
    "            f\"{case_title}에서 특허법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 특허법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 특허 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 특허 침해에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 특허법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '상표': [\n",
    "            f\"{case_title}에서 상표법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 상표법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 상표 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 상표 침해에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 상표법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '계약': [\n",
    "            f\"{case_title}에서 계약법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 계약법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 계약 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 계약 불이행에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 계약법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '부동산': [\n",
    "            f\"{case_title}에서 부동산법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 부동산법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 부동산 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 부동산 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 부동산법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '노동': [\n",
    "            f\"{case_title}에서 노동법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 노동법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 노동 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 노동 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 노동법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '형사': [\n",
    "            f\"{case_title}에서 형사법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 형사법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 형사 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 형사 사건에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 형사법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '민사': [\n",
    "            f\"{case_title}에서 민사법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 민사법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 민사 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 민사 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 민사법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '세금': [\n",
    "            f\"{case_title}에서 세법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 세법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 세금 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 세금 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 세법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '회사': [\n",
    "            f\"{case_title}에서 회사법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 회사법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 회사 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 회사 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 회사법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '환경': [\n",
    "            f\"{case_title}에서 환경법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 환경법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 환경 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 환경 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 환경법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '교통': [\n",
    "            f\"{case_title}에서 교통법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 교통법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 교통 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 교통 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 교통법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '가족': [\n",
    "            f\"{case_title}에서 가족법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 가족법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 가족 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 가족 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 가족법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '상속': [\n",
    "            f\"{case_title}에서 상속법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 상속법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 상속 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 상속 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 상속법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '지적재산권': [\n",
    "            f\"{case_title}에서 지적재산권법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 지적재산권법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 지적재산권 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 지적재산권 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 지적재산권법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '금융': [\n",
    "            f\"{case_title}에서 금융법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 금융법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 금융 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 금융 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 금융법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '해양': [\n",
    "            f\"{case_title}에서 해양법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 해양법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 해양 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 해양 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 해양법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '국제': [\n",
    "            f\"{case_title}에서 국제법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 국제법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 국제 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 국제 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 국제법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '보건': [\n",
    "            f\"{case_title}에서 보건법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 보건법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 보건 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 보건 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 보건법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '에너지': [\n",
    "            f\"{case_title}에서 에너지법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 에너지법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 에너지 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 에너지 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 에너지법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '군사': [\n",
    "            f\"{case_title}에서 군사법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 군사법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 군사 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 군사 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 군사법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '통신': [\n",
    "            f\"{case_title}에서 통신법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 통신법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 통신 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 통신 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 통신법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '교육': [\n",
    "            f\"{case_title}에서 교육법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 교육법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 교육 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 교육 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 교육법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '식품': [\n",
    "            f\"{case_title}에서 식품법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 식품법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 식품 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 식품 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 식품법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '항공': [\n",
    "            f\"{case_title}에서 항공법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 항공법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 항공 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 항공 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 항공법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '정보': [\n",
    "            f\"{case_title}에서 정보보호법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 정보보호법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 정보보호 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 정보보호 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 정보보호법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '미디어': [\n",
    "            f\"{case_title}에서 미디어법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 미디어법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 미디어 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 미디어 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 미디어법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '안전': [\n",
    "            f\"{case_title}에서 안전법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 안전법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 안전 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 안전 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 안전법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '공정거래': [\n",
    "            f\"{case_title}에서 공정거래법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 공정거래법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 공정거래 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 공정거래 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 공정거래법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '산업재해': [\n",
    "            f\"{case_title}에서 산업재해보상보험법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 산업재해보상보험법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 산업재해보상 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 산업재해보상 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 산업재해보상보험법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '보험': [\n",
    "            f\"{case_title}에서 보험법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 보험법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 보험 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 보험 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 보험법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '청소년': [\n",
    "            f\"{case_title}에서 청소년보호법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 청소년보호법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 청소년보호 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 청소년보호 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 청소년보호법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '공공기관': [\n",
    "            f\"{case_title}에서 공공기관운영법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 공공기관운영법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 공공기관운영 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 공공기관운영 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 공공기관운영법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '노인': [\n",
    "            f\"{case_title}에서 노인복지법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 노인복지법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 노인복지 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 노인복지 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 노인복지법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '아동': [\n",
    "            f\"{case_title}에서 아동복지법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 아동복지법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 아동복지 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 아동복지 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 아동복지법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '장애인': [\n",
    "            f\"{case_title}에서 장애인복지법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 장애인복지법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 장애인복지 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 장애인복지 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 장애인복지법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '동물': [\n",
    "            f\"{case_title}에서 동물보호법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 동물보호법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 동물보호 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 동물보호 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 동물보호법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '환경보호': [\n",
    "            f\"{case_title}에서 환경보호법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 환경보호법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 환경보호 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 환경보호 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 환경보호법 관련 판결은 무엇인가?\",\n",
    "        ],\n",
    "        '금융': [\n",
    "            f\"{case_title}에서 금융법 위반에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 금융법 위반 사건의 판결 결과는?\",\n",
    "            f\"{case_title}에서 금융 관련 법 위반에 대한 법원의 판단은?\",\n",
    "            f\"{case_title}에서 금융 분쟁에 대한 판결은 무엇인가?\",\n",
    "            f\"{case_title}의 금융법 관련 판결은 무엇인가?\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    for keyword, pattern_list in patterns.items():\n",
    "        if keyword in case_detail:\n",
    "            return random.choice(pattern_list)\n",
    "    \n",
    "    return f\"{case_title}에 대한 판결은 무엇인가?\"\n",
    "\n",
    "# 요약 함수\n",
    "def summarize_text(text):\n",
    "    inputs = summarizer_tokenizer([text], max_length=512, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = summarizer_model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 각 행에 대해 요약, 질문 생성 및 답변 생성\n",
    "results = []\n",
    "for index, row in df.iterrows():\n",
    "    context = row['court_detail']  # 적절한 컬럼 이름을 지정하세요\n",
    "    summarized_context = summarize_text(context) if pd.notnull(context) else \"\"\n",
    "    question = generate_question(row)\n",
    "\n",
    "    # 토크나이저를 사용하여 입력 텍스트를 인코딩\n",
    "    inputs = qa_tokenizer(question, summarized_context, return_tensors=\"pt\")\n",
    "\n",
    "    # 모델을 사용하여 답변 추론\n",
    "    with torch.no_grad():\n",
    "        outputs = qa_model(**inputs)\n",
    "\n",
    "    # 답변의 시작과 끝 위치를 추론\n",
    "    answer_start_index = torch.argmax(outputs.start_logits)\n",
    "    answer_end_index = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "    # 인덱스를 사용하여 원본 컨텍스트에서 답변 추출\n",
    "    answer = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start_index:answer_end_index]))\n",
    "\n",
    "    results.append({\n",
    "        \"Question\": question,\n",
    "        \"Context\": summarized_context,\n",
    "        \"Answer\": answer\n",
    "    })\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 결과를 엑셀 파일로 저장\n",
    "output_path = 'data/qa_results.xlsx'\n",
    "results_df.to_excel(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75379eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# 파일 경로\n",
    "file_path = 'data/context.xlsx'\n",
    "\n",
    "# 파일 로드\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['event_Title', 'court_detail']]\n",
    "\n",
    "# 특수 문자 제거 및 공백 제거\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 여러 공백을 하나로\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 특수 문자 제거\n",
    "    return text.strip()\n",
    "\n",
    "df['court_detail'] = df['court_detail'].apply(lambda x: clean_text(x) if pd.notnull(x) else '')\n",
    "\n",
    "# KoBART 요약 모델과 토크나이저 불러오기 (예시, 실제 사용 시 KoBART 경로로 변경)\n",
    "summarizer_tokenizer = AutoTokenizer.from_pretrained(\"hyunwoongko/kobart\")\n",
    "summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"hyunwoongko/kobart\")\n",
    "\n",
    "# KoELECTRA QA 모델과 토크나이저 불러오기\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "\n",
    "# 각 판례의 내용을 기반으로 질문 생성\n",
    "def generate_question(row):\n",
    "    case_title = row.get('event_Title', '이 사건')\n",
    "    case_detail = row.get('court_detail', '')\n",
    "\n",
    "    keywords = ['저작권', '특허', '상표', '계약', '부동산', '노동', '형사', '민사', '세금', '회사', '환경', '교통', '가족', '상속', '지적재산권', '금융', '해양', '국제', '보건', '에너지', '군사', '통신', '교육', '식품', '항공', '정보', '미디어', '안전', '공정거래', '산업재해', '보험', '청소년', '공공기관', '노인', '아동', '장애인', '동물', '환경보호']\n",
    "\n",
    "    for keyword in keywords:\n",
    "        if keyword in case_detail:\n",
    "            return f\"{case_title}에서 {keyword}법 위반에 대한 판결은 무엇인가?\"\n",
    "\n",
    "    return f\"{case_title}에 대한 판결은 무엇인가?\"\n",
    "\n",
    "# 요약 함수\n",
    "def summarize_text(text):\n",
    "    inputs = summarizer_tokenizer([text], max_length=512, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = summarizer_model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 각 행에 대해 요약, 질문 생성 및 답변 생성\n",
    "results = []\n",
    "for index, row in df.iterrows():\n",
    "    context = row['court_detail']\n",
    "    summarized_context = summarize_text(context) if pd.notnull(context) else \"\"\n",
    "    question = generate_question(row)\n",
    "    \n",
    "    print(f\"Original Context: {context}\")\n",
    "    print(f\"Summarized Context: {summarized_context}\")\n",
    "    print(f\"Generated Question: {question}\")\n",
    "\n",
    "    if summarized_context:\n",
    "        # 토크나이저를 사용하여 입력 텍스트를 인코딩\n",
    "        inputs = qa_tokenizer(question, summarized_context, return_tensors=\"pt\")\n",
    "\n",
    "        # 모델을 사용하여 답변 추론\n",
    "        with torch.no_grad():\n",
    "            outputs = qa_model(**inputs)\n",
    "\n",
    "        # 답변의 시작과 끝 위치를 추론\n",
    "        answer_start_index = torch.argmax(outputs.start_logits)\n",
    "        answer_end_index = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "        # 인덱스를 사용하여 원본 컨텍스트에서 답변 추출\n",
    "        answer = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start_index:answer_end_index]))\n",
    "    else:\n",
    "        answer = \"\"\n",
    "\n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    results.append({\n",
    "        \"Question\": question,\n",
    "        \"Context\": summarized_context,\n",
    "        \"Answer\": answer\n",
    "    })\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 결과를 엑셀 파일로 저장\n",
    "output_path = 'data/qa_results.xlsx'\n",
    "results_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe4082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 파일에서 데이터셋 불러오기\n",
    "dataset = load_from_disk(\"data/dataset\")\n",
    "\n",
    "# RAG 모델과 토크나이저 불러오기\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-nq\",\n",
    "    index_name=\"exact\",\n",
    "    use_dummy_dataset=False,\n",
    "    passages_path=None,  # passages_path를 None으로 설정하고 아래에서 설정\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\")\n",
    "\n",
    "# 인덱싱할 데이터 준비\n",
    "passages = dataset[\"text\"]\n",
    "\n",
    "# retriever에 passages 추가\n",
    "retriever.index.index_data(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d67b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 임베딩을 저장한 파일 로드\n",
    "embeddings_df = pd.read_excel('data/embeddings_data.xlsx')\n",
    "df = pd.read_excel('data/law_recomended.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.index.add(embeddings_df['court_detail'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c10ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f564da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "my_id = 'realstone.in'\n",
    "org = '400201' # 대법원\n",
    "JO = ['민사', '형사'] #%EB%AF%BC%EB%B2%95\n",
    "base_url = f'https://www.law.go.kr/DRF/lawSearch.do?OC={my_id}&target=prec&type=XML&display=100'\n",
    "event_base_url = 'http://www.law.go.kr'\n",
    "\n",
    "'''\n",
    "https://www.law.go.kr/DRF/lawSearch.do?OC=realstone.in&target=prec&type=XML&display=100&page=1&search=3&org=400201&JO=%EB%AF%BC%EB%B2%95\n",
    "'''\n",
    "\n",
    "# 사건명 사건번호 사건종류명 판례일련번호 판례상세링크 법원명\n",
    "\n",
    "\n",
    "page=1\n",
    "url = f'{base_url}&page={page}'#&org={org}''&JO={JO}\n",
    "event_Title = []\n",
    "event_num = []\n",
    "event_type = []\n",
    "jPrecedent_num = []\n",
    "jPrecedent_link_get = []\n",
    "jPrecedent_link_repl = []\n",
    "court_name = []\n",
    "court_num = []\n",
    "court_detail = []\n",
    "court_reference = []\n",
    "court_Decision = []\n",
    "\n",
    "response = requests.get(url)\n",
    "title_XML = response.text\n",
    "title_soup = BeautifulSoup(title_XML, 'xml') \n",
    "total_page = int(title_soup.select('totalCnt')[0].string)//100+1\n",
    "\n",
    "for page in tqdm(range(5000)) : #range(1, total_page+1)) :\n",
    "    url = f'{base_url}&page={page}'\n",
    "    response = requests.get(url)\n",
    "    title_XML = response.text\n",
    "    title_soup = BeautifulSoup(title_XML, 'xml') \n",
    "\n",
    "    for i in range(len(title_soup.select('사건명'))) :\n",
    "        event_Title.append(title_soup.select('사건명')[i].string)\n",
    "        event_num.append(title_soup.select('사건번호')[i].string)\n",
    "        event_type.append(title_soup.select('사건종류명')[i].string)\n",
    "        jPrecedent_num.append(title_soup.select('판례일련번호')[i].string)\n",
    "        jPrecedent_link_get.append(title_soup.select('판례상세링크')[i].string)\n",
    "        court_name.append(title_soup.select('법원명')[i].string)\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(jPrecedent_link_get))) :\n",
    "    jPrecedent_link_repl.append(jPrecedent_link_get[i][:-10].replace('HTML', 'XML'))\n",
    "    event_url = f'{event_base_url}{jPrecedent_link_repl[i]}'\n",
    "    event_response = requests.get(event_url)\n",
    "    main_XMLevent = event_response.text\n",
    "    main_soup = BeautifulSoup(main_XMLevent, 'xml')\n",
    "    #print(event_url)\n",
    "    #court_num.append(main_soup.select('사건번호')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_detail.append(main_soup.select('판례내용')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_reference.append(main_soup.select('참조조문')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_Decision.append(main_soup.select('판시사항')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'event_Title':event_Title, 'event_num':event_num, 'event_type':event_type, 'jPrecedent_num':jPrecedent_num, \\\n",
    "                  'jPrecedent_link_get':jPrecedent_link_get, 'court_name':court_name, \\\n",
    "                   'court_detail':court_detail, 'court_reference':court_reference, 'court_Decision':court_Decision})\n",
    "\n",
    "df.to_excel('data/court_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('court_title.csv', encoding='cp949')\n",
    "event_Title = list(df['event_Title'][:4999])\n",
    "event_num = list(df['event_num'][:4999])\n",
    "event_type = list(df['event_type'][:4999])\n",
    "jPrecedent_num = list(df['jPrecedent_num'][:4999])\n",
    "jPrecedent_link_get = list(df['jPrecedent_link_get'][:4999])\n",
    "court_name = list(df['court_name'][:4999])\n",
    "\n",
    "df = pd.DataFrame({'event_Title':event_Title, 'event_num':event_num, 'event_type':event_type, 'jPrecedent_num':jPrecedent_num, \\\n",
    "                  'jPrecedent_link_get':jPrecedent_link_get, 'court_name':court_name, \\\n",
    "                  'court_num':court_num, 'court_detail':court_detail, 'court_reference':court_reference, 'court_Decision':court_Decision})\n",
    "#df1 = pd.DataFrame()\n",
    "\n",
    "df.to_excel('court_title.xlsx')\n",
    "#df1.to_csv('court_detail.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ee3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('court_title.csv', encoding='cp949')\n",
    "event_Title = list(df['event_Title'][:5000])\n",
    "event_num = list(df['event_num'][:5000])\n",
    "event_type = list(df['event_type'][:5000])\n",
    "jPrecedent_num = list(df['jPrecedent_num'][:5000])\n",
    "jPrecedent_link_get = list(df['jPrecedent_link_get'][:5000])\n",
    "court_name = list(df['court_name'][:5000])\n",
    "df = pd.DataFrame({'event_Title':event_Title, 'event_num':event_num, 'event_type':event_type, 'jPrecedent_num':jPrecedent_num, \\\n",
    "                  'jPrecedent_link_get':jPrecedent_link_get, 'court_name':court_name, \\\n",
    "                   'court_detail':court_detail, 'court_reference':court_reference, 'court_Decision':court_Decision})\n",
    "\n",
    "df.to_excel('data/court_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('court_title.csv', encoding='cp949')\n",
    "event_Title = list(df['event_Title'][:5000])\n",
    "event_num = list(df['event_num'][:5000])\n",
    "event_type = list(df['event_type'][:5000])\n",
    "jPrecedent_num = list(df['jPrecedent_num'][:5000])\n",
    "jPrecedent_link_get = list(df['jPrecedent_link_get'][:5000])\n",
    "court_name = list(df['court_name'][:5000])\n",
    "\n",
    "jPrecedent_link_repl = []\n",
    "\n",
    "court_detail = []\n",
    "court_reference = []\n",
    "court_Decision = []\n",
    "\n",
    "for i in tqdm(range(5000)) :\n",
    "    jPrecedent_link_repl.append(jPrecedent_link_get[i][:-10].replace('HTML', 'XML'))\n",
    "    event_url = f'{event_base_url}{jPrecedent_link_repl[i]}'\n",
    "    event_response = requests.get(event_url)\n",
    "    main_XMLevent = event_response.text\n",
    "    main_soup = BeautifulSoup(main_XMLevent, 'xml')\n",
    "    #print(event_url)\n",
    "    #court_num.append(main_soup.select('사건번호')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_detail.append(main_soup.select('판례내용')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_reference.append(main_soup.select('참조조문')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_Decision.append(main_soup.select('판시사항')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    \n",
    "df = pd.DataFrame({'event_Title':event_Title, 'event_num':event_num, 'event_type':event_type, 'jPrecedent_num':jPrecedent_num, \\\n",
    "                  'jPrecedent_link_get':jPrecedent_link_get, 'court_name':court_name, \\\n",
    "                   'court_detail':court_detail, 'court_reference':court_reference, 'court_Decision':court_Decision})\n",
    "\n",
    "df.to_excel('data/court_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'event_Title':event_Title[:5000], 'event_num':event_num[:5000], 'event_type':event_type[:5000], 'jPrecedent_num':jPrecedent_num[:5000], \\\n",
    "                  'jPrecedent_link_get':jPrecedent_link_get[:5000], 'court_name':court_name, \\\n",
    "                   'court_detail':court_detail, 'court_reference':court_reference, 'court_Decision':court_Decision})\n",
    "\n",
    "df.to_excel('data/court_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e964ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "df = pd.read_csv('court_test.csv', encoding='CP949')\n",
    "text = df['court_detail'].replace('\\n', '').tolist()\n",
    "\n",
    "# KC-BERT 모델과 토크나이저 로드\n",
    "model_name = \"beomi/kcbert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 문장 임베딩 함수 정의\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # [CLS] 토큰의 출력 사용\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding.detach().numpy()\n",
    "\n",
    "embedding = [get_sentence_embedding(i) for i in tqdm(text)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df86b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install elasticsearch\n",
    "#!pip install datasets\n",
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagRetriever, RagTokenizer, RagSequenceForGeneration, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "import torch\n",
    "\n",
    "# 판례 데이터 로드\n",
    "df = pd.read_csv('court_test.csv', encoding='CP949')\n",
    "df['title'] = df['event_Title']  # 'court_title' 컬럼을 'title'로 사용\n",
    "df['text'] = df['court_detail']\n",
    "documents = df[['title', 'text']].to_dict(orient='records')  # 'title'과 'text' 컬럼을 포함하는 리스트로 변환\n",
    "\n",
    "# 데이터셋으로 변환\n",
    "dataset = Dataset.from_dict({\n",
    "    \"title\": [doc['title'] for doc in documents],\n",
    "    \"text\": [doc['text'] for doc in documents]\n",
    "})\n",
    "\n",
    "# DPRContextEncoder 모델과 토크나이저 로드\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "# 최대 길이 설정 (512 토큰)\n",
    "max_length = context_tokenizer.model_max_length\n",
    "\n",
    "# 텍스트 임베딩 생성 함수\n",
    "def embed_texts(batch):\n",
    "    inputs = context_tokenizer(batch['text'], truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        embeddings = context_encoder(**inputs).pooler_output\n",
    "    batch['embeddings'] = embeddings.numpy().tolist()  # NumPy 배열을 리스트로 변환\n",
    "    return batch\n",
    "\n",
    "# 텍스트 임베딩 생성 및 데이터셋에 추가\n",
    "dataset = dataset.map(embed_texts, batched=True, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2feb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 저장\n",
    "dataset_path = 'dataset'\n",
    "dataset.save_to_disk(dataset_path)\n",
    "\n",
    "# FAISS 인덱스 생성 및 저장\n",
    "dataset.add_faiss_index(column='embeddings')\n",
    "index_path = 'index'\n",
    "dataset.get_index('embeddings').save(index_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 모델과 토크나이저 로드\n",
    "tokenizer = RagTokenizer.from_pretrained('facebook/rag-sequence-nq')\n",
    "model = RagSequenceForGeneration.from_pretrained('facebook/rag-sequence-nq')\n",
    "\n",
    "# RAG Retriever 설정\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    'facebook/rag-sequence-nq',\n",
    "    index_name=\"custom\",\n",
    "    passages_path=dataset_path,  # passages_path를 dataset_path로 설정\n",
    "    index_path=index_path  # index_path 설정\n",
    ")\n",
    "\n",
    "# 사용자 질문에 대한 답변 생성\n",
    "query = input(\"질문을 입력하세요: \")\n",
    "input_ids = tokenizer(query, return_tensors='pt')['input_ids']\n",
    "\n",
    "try:\n",
    "    generated = model.generate(input_ids, retriever=retriever)\n",
    "    print(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad983dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 질문에 대한 답변 생성\n",
    "query = input(\"질문을 입력하세요: \")\n",
    "input_ids = tokenizer(query, return_tensors='pt')['input_ids']\n",
    "\n",
    "try:\n",
    "    generated = model.generate(input_ids, retriever=retriever)\n",
    "    print(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fce7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드 확인\n",
    "loaded_dataset = load_from_disk(dataset_path)\n",
    "print(loaded_dataset)\n",
    "\n",
    "# 인덱스 로드 확인\n",
    "loaded_index = loaded_dataset.get_index('embeddings')\n",
    "print(loaded_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958441d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f13607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8621c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02916e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5faa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5761ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Komoran\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('/content/court.csv', encoding='CP949')\n",
    "komoran = Komoran()\n",
    "text = df['court_detail'].str.replace(\"\\n\", '')\n",
    "\n",
    "# 텍스트를 형태소 단위로 나누기\n",
    "processed_text = [komoran.morphs(sentence) for sentence in tqdm(text, desc=\"텍스트 형태소 단위로 나누기\")]\n",
    "\n",
    "# 전처리된 텍스트 저장\n",
    "with open('/content/court_processed_text.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_text, f)\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "model = Word2Vec(processed_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save('/content/civil_word2vec_model.bin')\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer(tokenizer=komoran.morphs, max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(df['court_detail'])\n",
    "\n",
    "# 문장 벡터 생성 함수\n",
    "def get_sentence_vector(sentence, model, tfidf):\n",
    "    words = komoran.morphs(sentence)\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word] * tfidf.idf_[tfidf.vocabulary_.get(word, 0)])\n",
    "    \n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    return sentence_vector\n",
    "\n",
    "# 전체 문서 벡터 생성 및 저장\n",
    "sentence_vectors = np.array([get_sentence_vector(sentence, model, tfidf) for sentence in tqdm(df['court_detail'], desc=\"문서 벡터 생성\")])\n",
    "with open('/content/sentence_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(sentence_vectors, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00cb28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Komoran\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 저장된 모델과 매트릭스 불러오기\n",
    "with open('civil_law/civil_word2vec_model.bin', 'rb') as f:\n",
    "    word2vec_model = pickle.load(f)\n",
    "\n",
    "with open('civil_law/court_processed_text.pkl', 'rb') as f:\n",
    "    court_processed_text = pickle.load(f)\n",
    "\n",
    "\n",
    "    \n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('civil_law/court.csv', encoding='CP949')\n",
    "komoran = Komoran()\n",
    "text = df['court_detail'].str.replace(\"\\n\", '')\n",
    "\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer(tokenizer=komoran.morphs, max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(text)\n",
    "\n",
    "# 문장 벡터 생성 함수\n",
    "def get_sentence_vector(sentence, model, tfidf):\n",
    "    words = komoran.morphs(sentence)\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word] * tfidf.idf_[tfidf.vocabulary_.get(word, 0)])\n",
    "    \n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    return sentence_vector\n",
    "\n",
    "# 전체 문서 벡터 생성 및 저장\n",
    "sentence_vectors = np.array([get_sentence_vector(sentence, model, tfidf) for sentence in tqdm(df['court_detail'], desc=\"문서 벡터 생성\")])\n",
    "with open('/content/sentence_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(sentence_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d549288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 벡터 생성 함수\n",
    "def get_sentence_vector(sentence, model, tfidf):\n",
    "    words = komoran.morphs(sentence)\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word] * tfidf.idf_[tfidf.vocabulary_.get(word, 0)])\n",
    "    \n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    return sentence_vector\n",
    "\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('civil_law/court.csv', encoding='CP949')\n",
    "komoran = Komoran()\n",
    "text = df['court_detail'].str.replace(\"\\n\", '')\n",
    "\n",
    "#tfidf_vocab = tfidf_vectorizer.vocabulary_\n",
    "#tfidf_idf = tfidf_vectorizer.idf_\n",
    "\n",
    "\n",
    "# 전체 문서 벡터 생성 및 저장\n",
    "sentence_vectors = np.array([get_sentence_vector(sentence, word2vec_model, tfidf) for sentence in tqdm(df['court_detail'], desc=\"문서 벡터 생성\")])\n",
    "with open('path/to/sentence_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(sentence_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b8c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a6f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac9709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c8bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c393e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a50db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738334a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf75d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb0eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4007912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from scipy.spatial.distance import jaccard\n",
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Komoran\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./court.csv', encoding='cp949')\n",
    "komoran = Komoran()\n",
    "text = df['court_detail'].str.replace(\"\\n\", '')\n",
    "\n",
    "# 텍스트를 형태소 단위로 나누기\n",
    "processed_text = [komoran.morphs(sentence) for sentence in text]\n",
    "\n",
    "# TF-IDF 벡터라이저 및 매트릭스 생성\n",
    "tfidf_vectorizer_unigram = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_matrix_unigram = tfidf_vectorizer_unigram.fit_transform([' '.join(sentence) for sentence in processed_text])\n",
    "\n",
    "tfidf_vectorizer_bigram = TfidfVectorizer(ngram_range=(1, 2))\n",
    "tfidf_matrix_bigram = tfidf_vectorizer_bigram.fit_transform([' '.join(sentence) for sentence in processed_text])\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "word2vec_model = Word2Vec(sentences=processed_text, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# 문장 임베딩 생성\n",
    "def get_sentence_embedding(model, sentence):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "sentence_embeddings = np.array([get_sentence_embedding(word2vec_model, sentence) for sentence in processed_text])\n",
    "\n",
    "# 유사도 계산 함수\n",
    "def calculate_similarity(matrix, method='cosine'):\n",
    "    if method == 'cosine':\n",
    "        return cosine_similarity(matrix)\n",
    "    elif method == 'euclidean':\n",
    "        return -euclidean_distances(matrix)  # 유사도는 거리가 작을수록 크기 때문에 음수로 변환\n",
    "    elif method == 'jaccard':\n",
    "        similarities = np.zeros((matrix.shape[0], matrix.shape[0]))\n",
    "        for i in range(matrix.shape[0]):\n",
    "            for j in range(matrix.shape[0]):\n",
    "                if i != j:\n",
    "                    similarities[i, j] = 1 - jaccard(matrix[i].toarray()[0], matrix[j].toarray()[0])\n",
    "        return similarities\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "# 유사도 계산\n",
    "tfidf_unigram_similarity_cosine = calculate_similarity(tfidf_matrix_unigram, 'cosine')\n",
    "tfidf_bigram_similarity_cosine = calculate_similarity(tfidf_matrix_bigram, 'cosine')\n",
    "word2vec_similarity_cosine = calculate_similarity(sentence_embeddings, 'cosine')\n",
    "\n",
    "tfidf_unigram_similarity_euclidean = calculate_similarity(tfidf_matrix_unigram, 'euclidean')\n",
    "tfidf_bigram_similarity_euclidean = calculate_similarity(tfidf_matrix_bigram, 'euclidean')\n",
    "word2vec_similarity_euclidean = calculate_similarity(sentence_embeddings, 'euclidean')\n",
    "\n",
    "tfidf_unigram_similarity_jaccard = calculate_similarity(tfidf_matrix_unigram, 'jaccard')\n",
    "tfidf_bigram_similarity_jaccard = calculate_similarity(tfidf_matrix_bigram, 'jaccard')\n",
    "# Jaccard 유사도는 Word2Vec 임베딩에는 사용하지 않음 (이진 데이터에 적합)\n",
    "\n",
    "# 가장 높은 평균 유사도를 가지는 방법 선택\n",
    "similarities = {\n",
    "    'TF-IDF Unigram Cosine': np.mean(tfidf_unigram_similarity_cosine),\n",
    "    'TF-IDF Bigram Cosine': np.mean(tfidf_bigram_similarity_cosine),\n",
    "    'Word2Vec Cosine': np.mean(word2vec_similarity_cosine),\n",
    "    'TF-IDF Unigram Euclidean': np.mean(tfidf_unigram_similarity_euclidean),\n",
    "    'TF-IDF Bigram Euclidean': np.mean(tfidf_bigram_similarity_euclidean),\n",
    "    'Word2Vec Euclidean': np.mean(word2vec_similarity_euclidean),\n",
    "    'TF-IDF Unigram Jaccard': np.mean(tfidf_unigram_similarity_jaccard),\n",
    "    'TF-IDF Bigram Jaccard': np.mean(tfidf_bigram_similarity_jaccard)\n",
    "}\n",
    "\n",
    "best_method = max(similarities, key=similarities.get)\n",
    "best_similarity_matrix = None\n",
    "\n",
    "if 'Unigram' in best_method:\n",
    "    best_model = tfidf_vectorizer_unigram\n",
    "    if 'Cosine' in best_method:\n",
    "        best_similarity_matrix = tfidf_unigram_similarity_cosine\n",
    "    elif 'Euclidean' in best_method:\n",
    "        best_similarity_matrix = tfidf_unigram_similarity_euclidean\n",
    "    elif 'Jaccard' in best_method:\n",
    "        best_similarity_matrix = tfidf_unigram_similarity_jaccard\n",
    "elif 'Bigram' in best_method:\n",
    "    best_model = tfidf_vectorizer_bigram\n",
    "    if 'Cosine' in best_method:\n",
    "        best_similarity_matrix = tfidf_bigram_similarity_cosine\n",
    "    elif 'Euclidean' in best_method:\n",
    "        best_similarity_matrix = tfidf_bigram_similarity_euclidean\n",
    "    elif 'Jaccard' in best_method:\n",
    "        best_similarity_matrix = tfidf_bigram_similarity_jaccard\n",
    "elif 'Word2Vec' in best_method:\n",
    "    best_model = word2vec_model\n",
    "    if 'Cosine' in best_method:\n",
    "        best_similarity_matrix = word2vec_similarity_cosine\n",
    "    elif 'Euclidean' in best_method:\n",
    "        best_similarity_matrix = word2vec_similarity_euclidean\n",
    "\n",
    "print(f\"Best method: {best_method}\")\n",
    "\n",
    "# 모델과 유사도 매트릭스 저장\n",
    "with open('best_model_civil_with_komoran.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "with open('best_similarity_matrix_civil_with_komoran.pkl', 'wb') as f:\n",
    "    pickle.dump(best_similarity_matrix, f)\n",
    "\n",
    "# 전처리된 텍스트 저장\n",
    "with open('court_processed_text_civil_with_komoran.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_text, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bbd81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ee2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32484e1",
   "metadata": {},
   "source": [
    "코드 설명\n",
    "\n",
    "데이터 로드 및 전처리:\n",
    "\n",
    "데이터를 로드하고 줄바꿈 문자를 제거합니다.\n",
    "\n",
    "텍스트를 형태소 단위로 나눕니다.\n",
    "\n",
    "TF-IDF 벡터라이저 및 매트릭스 생성:\n",
    "\n",
    "1-그램 및 2-그램을 사용하여 TF-IDF 매트릭스를 생성합니다.\n",
    "\n",
    "Word2Vec 모델 학습 및 문장 임베딩 생성:\n",
    "\n",
    "Word2Vec 모델을 학습하고, 각 문장을 임베딩 벡터로 변환합니다.\n",
    "\n",
    "유사도 계산 함수:\n",
    "\n",
    "주어진 매트릭스에 대해 코사인 유사도를 계산합니다.\n",
    "\n",
    "유사도 계산:\n",
    "\n",
    "각각의 방법에 대해 유사도를 계산합니다.\n",
    "\n",
    "가장 높은 평균 유사도를 가지는 방법 선택:\n",
    "\n",
    "각 방법의 평균 유사도를 계산하고, 가장 높은 유사도를 가지는 방법을 선택합니다.\n",
    "\n",
    "선택된 모델과 유사도 매트릭스 저장:\n",
    "\n",
    "선택된 모델과 유사도 매트릭스를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from konlpy.tag import Komoran\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#mecab = Mecab('C:/mecab/mecab-ko-dic')\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('civil law/court.csv', encoding='cp949')\n",
    "komoran = Komoran()\n",
    "text = df['court_detail'].str.replace(\"\\n\", '')\n",
    "\n",
    "# 텍스트를 형태소 단위로 나누기\n",
    "processed_text = []\n",
    "for i in text :\n",
    "    processed_text.append(komoran.morphs(i))\n",
    "#= [komoran.morphs(sentence) for sentence in text]\n",
    "\n",
    "# 전처리된 텍스트 저장\n",
    "with open('court_processed_text.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_text, f)\n",
    "\n",
    "model = Word2Vec(processed_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.save('civil_word2vec_model.bin')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e748ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from konlpy.tag import Komoran\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    # 문장을 토큰화하여 단어 리스트를 얻습니다.\n",
    "    komoran = Komoran()\n",
    "    words = komoran.morphs(sentence)\n",
    "    # 단어 리스트에서 각 단어의 벡터를 가져옵니다.\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])\n",
    "    \n",
    "    # 단어 벡터가 없다면, 제로 벡터를 반환합니다.\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    # 단어 벡터들의 평균을 계산하여 문장 벡터를 얻습니다.\n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    \n",
    "    return sentence_vector\n",
    "\n",
    "\n",
    "# 저장된 모델과 매트릭스 불러오기\n",
    "with open('civil_word2vec_model.bin', 'rb') as f:\n",
    "    word2vec_model = pickle.load(f)\n",
    "\n",
    "with open('court_processed_text.pkl', 'rb') as f:\n",
    "    court_processed_text = pickle.load(f)\n",
    "\n",
    "# 처리된 텍스트가 리스트 형태라면, 이를 문자열로 변환\n",
    "processed_sentences = [' '.join(sentence) if isinstance(sentence, list) else sentence for sentence in tqdm(court_processed_text)]\n",
    "\n",
    "# 처리된 텍스트를 문장 벡터로 변환\n",
    "sentence_vectors = np.array([get_sentence_vector(sentence, word2vec_model) for sentence in tqdm(processed_sentences)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sentence_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(sentence_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dace606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Komoran\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "#===========================================함수========================================\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    # 문장을 토큰화하여 단어 리스트를 얻습니다.\n",
    "    komoran = Komoran()\n",
    "    words = komoran.morphs(sentence)\n",
    "    # 단어 리스트에서 각 단어의 벡터를 가져옵니다.\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])\n",
    "    \n",
    "    # 단어 벡터가 없다면, 제로 벡터를 반환합니다.\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    # 단어 벡터들의 평균을 계산하여 문장 벡터를 얻습니다.\n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    \n",
    "    return sentence_vector\n",
    "\n",
    "def recommend_court(input_sentence, top_n=5):    \n",
    "    # 입력 문장을 TF-IDF 변환\n",
    "    input_vector = get_sentence_vector(input_sentence, word2vec_model)\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    similarities = cosine_similarity([input_vector], sentence_vectors)[0]\n",
    "    \n",
    "    # 유사도 순으로 정렬된 상위 n개 인덱스 추출\n",
    "    most_similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    \n",
    "    # 유사도 0인 경우 처리\n",
    "    recommendations = []\n",
    "    for idx in most_similar_indices:\n",
    "        if similarities[idx] == 0:\n",
    "            break\n",
    "        recommendations.append((df.iloc[idx]['event_Title'], \\\n",
    "                                df.iloc[idx]['event_num'], \\\n",
    "                                df.iloc[idx]['event_type'], \\\n",
    "                                df.iloc[idx]['jPrecedent_num'], \\\n",
    "                                df.iloc[idx]['court_name'], \\\n",
    "                                df.iloc[idx]['court_num'], \\\n",
    "                                df.iloc[idx]['court_detail'], \\\n",
    "                                df.iloc[idx]['court_reference'], \\\n",
    "                                df.iloc[idx]['court_Decision'], \\\n",
    "                                similarities[idx]*100))\n",
    "    \n",
    "    if len(recommendations) == 0:\n",
    "        return \"유사한 판례를 찾을 수 없습니다.\"\n",
    "    else:\n",
    "        return recommendations\n",
    "    \n",
    "    \n",
    "    \n",
    "#===========================================파일========================================\n",
    "    \n",
    "with open('civil_word2vec_model.bin', 'rb') as f:\n",
    "    word2vec_model = pickle.load(f)\n",
    "    \n",
    "with open('sentence_vectors.pkl', 'rb') as f:\n",
    "    sentence_vectors = pickle.load(f)\n",
    "\n",
    "df = pd.read_csv('civil_law\\court.csv', encoding='cp949')\n",
    " \n",
    "    \n",
    "    \n",
    "#===========================================실행========================================\n",
    "\n",
    "court_input = str(input('상황을 작성해 주세요 (자세히 작성 시 검색에 도움이 됩니다.) :'))\n",
    "court_detail_OX = str(input('판례 및 판시 사항을 보시겠습니까?(Y/N) : '))\n",
    "input_sentence = court_input\n",
    "\n",
    "# 판례\n",
    "recommended_court = recommend_court(input_sentence)\n",
    "if isinstance(recommended_court, str):\n",
    "    print(recommended_court)\n",
    "else:\n",
    "    for event_Title, event_num, event_type, jPrecedent_num, court_name, court_num, court_detail, court_reference, court_Decision, similarity  \\\n",
    "    in recommended_court:\n",
    "        print('==================================================================================================================')\n",
    "        if (court_detail_OX == 'N') or (court_detail_OX == 'n') :\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\\n",
    "                    \\n판례 일련번호 : {jPrecedent_num} \\\n",
    "                    \\n법원 : {court_name} \\n사건 번호 : {court_num} \\n\\n참조조문 \\n{court_reference}\\n\\n\\n\")\n",
    "        else :\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\\n",
    "                    \\n판례 일련번호 : {jPrecedent_num} \\\n",
    "                    \\n법원 : {court_name} \\n 사건 번호 : {court_num} \\n\\n판례 내용 :\\n{court_detail} \\n\\n참조조문 \\n{court_reference} \\\n",
    "                    \\n\\n판시사항 :\\n{court_Decision}\\n\\n\\n\")\n",
    "        print('==================================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd0e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa7721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51980917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bacbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd6d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542146a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6374018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea78054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8449861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33eb205",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b927cac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'read_excel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/law_recomended.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# 추출한 텍스트 데이터 파일\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# TF-IDF 벡터화\u001b[39;00m\n\u001b[0;32m     10\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read_excel'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_excel('./data/law_recomended.xlsx') # 추출한 텍스트 데이터 파일\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['court_detail'])\n",
    "\n",
    "# K-Means 군집화\n",
    "kmeans = KMeans(n_clusters=7, random_state=42) # 원하는 군집 수 설정\n",
    "data['cluster'] = kmeans.fit_predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd463a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58445777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce7ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_court(input_sentence, top_n=5):\n",
    "    # 입력 문장을 Okt를 사용하여 전처리\n",
    "    okt = Okt()\n",
    "    processed_input = ' '.join(okt.morphs(input_sentence))\n",
    "    \n",
    "    # 입력 문장을 TF-IDF 변환\n",
    "    input_tfidf = tfidf_vectorizer.transform([processed_input])\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    cosine_similarities = cosine_similarity(input_tfidf, tfidf_matrix).flatten()\n",
    "    \n",
    "    # 유사도 순으로 정렬된 상위 n개 인덱스 추출\n",
    "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # 유사도 0인 경우 처리\n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        if cosine_similarities[idx] == 0:\n",
    "            break\n",
    "        recommendations.append((df.iloc[idx]['event_Title'], \\\n",
    "                                df.iloc[idx]['event_num'], \\\n",
    "                                df.iloc[idx]['event_type'], \\\n",
    "                                df.iloc[idx]['jPrecedent_num'], \\\n",
    "                                df.iloc[idx]['court_name'], \\\n",
    "                                df.iloc[idx]['court_num'], \\\n",
    "                                df.iloc[idx]['court_detail'], \\\n",
    "                                df.iloc[idx]['court_reference'], \\\n",
    "                                df.iloc[idx]['court_Decision'], \\\n",
    "                                cosine_similarities[idx]*100))\n",
    "    \n",
    "    if len(recommendations) == 0:\n",
    "        return \"유사한 판례를 찾을 수 없습니다.\"\n",
    "    else:\n",
    "        return recommendations\n",
    "\n",
    "court_input = str(input('상황을 작성해 주세요 (자세히 작성 시 검색에 도움이 됩니다.) :'))\n",
    "court_detail_OX = str(input('판례 및 판시 사항을 보시겠습니까?(Y/N) : '))\n",
    "input_sentence = court_input\n",
    "\n",
    "# 판례\n",
    "recommended_court = recommend_court(input_sentence)\n",
    "if isinstance(recommended_court, str):\n",
    "    print(recommended_court)\n",
    "else:\n",
    "    for event_Title, event_num, event_type, jPrecedent_num, court_name, court_num, court_detail, court_reference, court_Decision, similarity  \\\n",
    "    in recommended_court:\n",
    "        \n",
    "        if (court_detail_OX == 'N') or (court_detail_OX == 'n') :\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\\n",
    "                    \\n판례 일련번호 : {jPrecedent_num} \\\n",
    "                    \\n법원 : {court_name} \\n사건 번호 : {court_num} \\n\\n참조조문 \\n{court_reference}\\n\\n\\n\")\n",
    "        else :\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\\n",
    "                    \\n판례 일련번호 : {jPrecedent_num} \\\n",
    "                    \\n법원 : {court_name} \\n 사건 번호 : {court_num} \\n\\n판례 내용 :\\n{court_detail} \\n\\n참조조문 \\n{court_reference} \\\n",
    "                    \\n\\n판시사항 :\\n{court_Decision}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a420e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e57f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fe987fc",
   "metadata": {},
   "source": [
    "# API 제공 판례 크롤링 (전체 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89693aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8d2f225bcb42cb80e6103b6827f6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5500e62ea9804000b14b2c68bcfd9a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87446 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 69>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m     court_reference\u001b[38;5;241m.\u001b[39mappend(main_soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m참조조문\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstring\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</br>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<br/>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     66\u001b[0m     court_Decision\u001b[38;5;241m.\u001b[39mappend(main_soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m판시사항\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstring\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</br>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<br/>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 69\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_Title\u001b[39m\u001b[38;5;124m'\u001b[39m:event_Title, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_num\u001b[39m\u001b[38;5;124m'\u001b[39m:event_num, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m'\u001b[39m:event_type, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjPrecedent_num\u001b[39m\u001b[38;5;124m'\u001b[39m:jPrecedent_num, \\\n\u001b[0;32m     70\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjPrecedent_link_get\u001b[39m\u001b[38;5;124m'\u001b[39m:jPrecedent_link_get, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourt_name\u001b[39m\u001b[38;5;124m'\u001b[39m:court_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourt_num\u001b[39m\u001b[38;5;124m'\u001b[39m:court_num, \\\n\u001b[0;32m     71\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourt_detail\u001b[39m\u001b[38;5;124m'\u001b[39m:court_detail, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourt_reference\u001b[39m\u001b[38;5;124m'\u001b[39m:court_reference, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourt_Decision\u001b[39m\u001b[38;5;124m'\u001b[39m:court_Decision})\n\u001b[0;32m     73\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/court_all_data_0712.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'DataFrame'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.request import urlopen\n",
    "from tqdm import trange\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import re\n",
    "\n",
    "my_id = 'realstone.in'\n",
    "#org = '400201' # 대법원\n",
    "#JO = '민법' #%EB%AF%BC%EB%B2%95\n",
    "base_url = f'https://www.law.go.kr/DRF/lawSearch.do?OC={my_id}&target=prec&type=XML&display=100'\n",
    "event_base_url = 'http://www.law.go.kr'\n",
    "\n",
    "'''\n",
    "https://www.law.go.kr/DRF/lawSearch.do?OC=realstone.in&target=prec&type=XML&display=100&page=1&search=3&org=400201&JO=%EB%AF%BC%EB%B2%95\n",
    "'''\n",
    "\n",
    "# 사건명 사건번호 사건종류명 판례일련번호 판례상세링크 법원명\n",
    "event_Title = []\n",
    "event_num = []\n",
    "event_type = []\n",
    "jPrecedent_num = []\n",
    "jPrecedent_link_get = []\n",
    "jPrecedent_link_repl = []\n",
    "court_name = []\n",
    "court_num = []\n",
    "court_detail = []\n",
    "court_reference = []\n",
    "court_Decision = []\n",
    "page=1\n",
    "url = f'{base_url}&page={page}'\n",
    "#&org={org}\n",
    "response = requests.get(url)\n",
    "title_XML = response.text\n",
    "title_soup = BeautifulSoup(title_XML, 'xml') \n",
    "\n",
    "total_page = int(title_soup.select('totalCnt')[0].string)//100+1\n",
    "\n",
    "for page in tqdm(range(1, total_page+1)) :\n",
    "    url = f'{base_url}&page={page}'\n",
    "    response = requests.get(url)\n",
    "    title_XML = response.text\n",
    "    title_soup = BeautifulSoup(title_XML, 'xml') \n",
    "\n",
    "    for i in range(len(title_soup.select('사건명'))) :\n",
    "        event_Title.append(title_soup.select('사건명')[i].string)\n",
    "        event_num.append(title_soup.select('사건번호')[i].string)\n",
    "        event_type.append(title_soup.select('사건종류명')[i].string)\n",
    "        jPrecedent_num.append(title_soup.select('판례일련번호')[i].string)\n",
    "        jPrecedent_link_get.append(title_soup.select('판례상세링크')[i].string)\n",
    "        court_name.append(title_soup.select('법원명')[i].string)\n",
    "        \n",
    "\n",
    "for i in tqdm(range(len(jPrecedent_link_get))) :\n",
    "    jPrecedent_link_repl.append(jPrecedent_link_get[i][:-10].replace('HTML', 'XML'))\n",
    "    event_url = f'{event_base_url}{jPrecedent_link_repl[i]}'\n",
    "    event_response = requests.get(event_url)\n",
    "    main_XMLevent = event_response.text\n",
    "    main_soup = BeautifulSoup(main_XMLevent, 'xml')\n",
    "    #print(event_url)\n",
    "    court_num.append(main_soup.select('사건번호')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_detail.append(main_soup.select('판례내용')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_reference.append(main_soup.select('참조조문')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_Decision.append(main_soup.select('판시사항')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "\n",
    "    \n",
    "df = pd.DataFrame({'event_Title':event_Title, 'event_num':event_num, 'event_type':event_type, 'jPrecedent_num':jPrecedent_num, \\\n",
    "                  'jPrecedent_link_get':jPrecedent_link_get, 'court_name':court_name, 'court_num':court_num, \\\n",
    "                   'court_detail':court_detail, 'court_reference':court_reference, 'court_Decision':court_Decision})\n",
    "\n",
    "df.to_excel('data/court_all_data_0712.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "279afd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reals\\AppData\\Local\\Temp\\ipykernel_14192\\4205941862.py:5: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  df.to_excel('data/court_all_data_0712.xlsx')\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'event_Title':event_Title, 'event_num':event_num, 'event_type':event_type, 'jPrecedent_num':jPrecedent_num, \\\n",
    "                  'jPrecedent_link_get':jPrecedent_link_get, 'court_name':court_name, 'court_num':court_num, \\\n",
    "                   'court_detail':court_detail, 'court_reference':court_reference, 'court_Decision':court_Decision})\n",
    "\n",
    "df.to_excel('data/court_all_data_0712.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c863a5b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Downloading numpy-2.0.0-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 60.9/60.9 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Using cached pandas-2.2.2-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Downloading numpy-2.0.0-cp39-cp39-win_amd64.whl (16.5 MB)\n",
      "   ---------------------------------------- 16.5/16.5 MB 38.6 MB/s eta 0:00:00\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "   --------------------------------------- 505.5/505.5 kB 16.0 MB/s eta 0:00:00\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   --------------------------------------- 345.4/345.4 kB 20.9 MB/s eta 0:00:00\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2021.3\n",
      "    Uninstalling pytz-2021.3:\n",
      "      Successfully uninstalled pytz-2021.3\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2024.1\n",
      "    Uninstalling tzdata-2024.1:\n",
      "      Successfully uninstalled tzdata-2024.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "Successfully installed numpy-2.0.0 pandas-2.2.2 python-dateutil-2.9.0.post0 pytz-2024.1 six-1.16.0 tzdata-2024.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\reals\\anaconda3\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "faiss-cpu 1.8.0.post1 requires numpy<2.0,>=1.0, but you have numpy 2.0.0 which is incompatible.\n",
      "jupyter-server 1.13.5 requires pywinpty<2; os_name == \"nt\", but you have pywinpty 2.0.2 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 2.0.0 which is incompatible.\n",
      "scipy 1.8.0 requires numpy<1.25.0,>=1.17.3, but you have numpy 2.0.0 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed89f111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\reals\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\reals\\AppData\\Local\\Temp\\ipykernel_14192\\1385292462.py\", line 4, in <cell line: 4>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\", line 77, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\reals\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\reals\\AppData\\Local\\Temp\\ipykernel_14192\\1385292462.py\", line 4, in <cell line: 4>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\", line 77, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\reals\\anaconda3\\lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.modules.pop('pandas', None)\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6]\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86244fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "'''\n",
    "event_Title \n",
    "event_num \n",
    "event_type \n",
    "jPrecedent_num \n",
    "jPrecedent_link_get \n",
    "court_name \n",
    "court_num \n",
    "court_detail \n",
    "court_reference \n",
    "court_Decision\n",
    "'''\n",
    "\n",
    "df = pd.DataFrame({'event_Title':event_Title, 'event_num':event_num, 'event_type':event_type, 'jPrecedent_num':jPrecedent_num, \\\n",
    "                  'jPrecedent_link_get':jPrecedent_link_get, 'court_name':court_name, 'court_num':court_num, \\\n",
    "                   'court_detail':court_detail, 'court_reference':court_reference, 'court_Decision':court_Decision})\n",
    "\n",
    "df.to_csv('court.csv',encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5196429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('court.csv', encoding='cp949')\n",
    "# 줄거리 텍스트 데이터 전처리\n",
    "okt = Okt()\n",
    "text = df['court_detail'].str.replace(\"\\n\", '')\n",
    "\n",
    "# 텍스트를 형태소 단위로 나누기\n",
    "processed_text = [' '.join(okt.morphs(sentence)) for sentence in text]\n",
    "\n",
    "# TF-IDF 벡터라이저 초기화 및 학습\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_text)\n",
    "\n",
    "# 모델과 TF-IDF 매트릭스 저장\n",
    "with open('court_tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "with open('court_tfidf_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix, f)\n",
    "\n",
    "# 전처리된 텍스트 저장\n",
    "with open('court_processed_text.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be005f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "df = pd.read_csv('court.csv', encoding='cp949')\n",
    "\n",
    "# 저장된 모델과 매트릭스 불러오기\n",
    "with open('civil law/court_tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "with open('civil law/court_tfidf_matrix.pkl', 'rb') as f:\n",
    "    tfidf_matrix = pickle.load(f)\n",
    "\n",
    "with open('civil law/court_processed_text.pkl', 'rb') as f:\n",
    "    processed_text = pickle.load(f)\n",
    "\n",
    "def recommend_court(input_sentence, top_n=5):\n",
    "    # 입력 문장을 Okt를 사용하여 전처리\n",
    "    okt = Okt()\n",
    "    processed_input = ' '.join(okt.morphs(input_sentence))\n",
    "    \n",
    "    # 입력 문장을 TF-IDF 변환\n",
    "    input_tfidf = tfidf_vectorizer.transform([processed_input])\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    cosine_similarities = cosine_similarity(input_tfidf, tfidf_matrix).flatten()\n",
    "    \n",
    "    # 유사도 순으로 정렬된 상위 n개 인덱스 추출\n",
    "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # 유사도 0인 경우 처리\n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        if cosine_similarities[idx] == 0:\n",
    "            break\n",
    "        recommendations.append((df.iloc[idx]['event_Title'], \\\n",
    "                                df.iloc[idx]['event_num'], \\\n",
    "                                df.iloc[idx]['event_type'], \\\n",
    "                                df.iloc[idx]['jPrecedent_num'], \\\n",
    "                                df.iloc[idx]['court_name'], \\\n",
    "                                df.iloc[idx]['court_num'], \\\n",
    "                                df.iloc[idx]['court_detail'], \\\n",
    "                                df.iloc[idx]['court_reference'], \\\n",
    "                                df.iloc[idx]['court_Decision'], \\\n",
    "                                cosine_similarities[idx]*100))\n",
    "    \n",
    "    if len(recommendations) == 0:\n",
    "        return \"유사한 판례를 찾을 수 없습니다.\"\n",
    "    else:\n",
    "        return recommendations\n",
    "\n",
    "court_input = str(input('상황을 작성해 주세요 (자세히 작성 시 검색에 도움이 됩니다.) :'))\n",
    "court_detail_OX = str(input('판례 및 판시 사항을 보시겠습니까?(Y/N) : '))\n",
    "input_sentence = court_input\n",
    "\n",
    "# 판례\n",
    "recommended_court = recommend_court(input_sentence)\n",
    "if isinstance(recommended_court, str):\n",
    "    print(recommended_court)\n",
    "else:\n",
    "    for event_Title, event_num, event_type, jPrecedent_num, court_name, court_num, court_detail, court_reference, court_Decision, similarity  \\\n",
    "    in recommended_court:\n",
    "        \n",
    "        if (court_detail_OX == 'N') or (court_detail_OX == 'n') :\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\\n",
    "                    \\n판례 일련번호 : {jPrecedent_num} \\\n",
    "                    \\n법원 : {court_name} \\n사건 번호 : {court_num} \\n\\n참조조문 \\n{court_reference}\\n\\n\\n\")\n",
    "        else :\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f}%)\\n사건 번호 : {event_num} \\n사건 종류 : {event_type}\\\n",
    "                    \\n판례 일련번호 : {jPrecedent_num} \\\n",
    "                    \\n법원 : {court_name} \\n 사건 번호 : {court_num} \\n\\n판례 내용 :\\n{court_detail} \\n\\n참조조문 \\n{court_reference} \\\n",
    "                    \\n\\n판시사항 :\\n{court_Decision}\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3c14d",
   "metadata": {},
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce8fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import pickle  # 모델 저장 및 로딩을 위해\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "def model(filepath, court_detail):\n",
    "    tfidf = TfidfVectorizer(tokenizer=Okt().morphs, max_features=500)\n",
    "    tfidf.fit(court_detail)\n",
    "    # 모델 저장\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(tfidf, f)\n",
    "'''\n",
    "TfidfVectorizer() 하이퍼 파라미터\n",
    "    min_df : 설정한 값보다 특정 Token의 df 값이 더 적게 나오면 벡터화 과정에서 제거\n",
    "    anlayzer : 분석 단위를 의미, ‘word’의 경우 간어 하나를 단위로, ‘char’는 문자 하나를 단위로\n",
    "    sublinear_tf : 문서의 단어 빈도수(tf:term frequency)에 대한 smoothing 여부를 설정\n",
    "    ngram_range : 빈도의 기본 단위를 어떤 범위의 n-gram으로 설정할 것인지를 보는 인자\n",
    "    max_features : 각 벡터의 최대 길이(특징의 길이)를 설정\n",
    "'''\n",
    "        \n",
    "        \n",
    "# 저장된 TF-IDF 모델을 불러와 사용하는 함수\n",
    "def load_tfidf_model(filepath='tfidf_model.pkl'):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def model_with_pretrained_tfidf(detail, num, court_detail):\n",
    "    tfidf = load_tfidf_model()  # 저장된 모델 불러오기\n",
    "    all_detail = court_detail + [detail]\n",
    "    tfidf_matrix = tfidf.transform(all_detail)  # fit_transform 대신 transform 사용\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    cosine_similar_indices = np.argsort(cosine_sim)[-num:][::-1]\n",
    "    return cosine_similar_indices\n",
    "\n",
    "court_detail = pd.read_csv('court.csv', encoding='cp949')['court_detail']\n",
    "path = 'tfidf_model.pkl'\n",
    "model(path, court_detail)\n",
    "\n",
    "'''\n",
    "detail = str(input('내용을 입력해 주세요 : '))#'새로 선임된 청산인을 기존 이사회가 해임시킬 수 있어?'\n",
    "num = int(input('몇 개의 판례를 보시겠습니까? (최대 5개) : '))\n",
    "if num == 1:\n",
    "    OX = str(input('판례 내용을 보시겠습니까?(O, X) : '))\n",
    "    print(f'질문 : \"{detail}\"')\n",
    "    index = model_with_pretrained_tfidf(detail, num, court_detail)\n",
    "    print(f'사건 번호 : {court_num[index[0][0]]}')\n",
    "    print(f'참조 조문 : {court_reference[index[0][0]]}')\n",
    "    print(f'판시 사항 : {court_Decision[index[0][0]]}')\n",
    "    if OX == 'O' :\n",
    "        print(f'유사한 판례 : {court_detail[index[0][0]]}')\n",
    "elif num <= 5:\n",
    "    OX = str(input('판례 내용을 보시겠습니까?(O, X) : '))\n",
    "    print(f'질문 : \"{detail}\"')\n",
    "    index = model_with_pretrained_tfidf(detail, num, court_detail)\n",
    "    for i in index[0][:num]:\n",
    "        print(f'사건 번호 : {court_num[index[0][i]]}')\n",
    "        print(f'참조 조문 : {court_reference[index[0][i]]}')\n",
    "        print(f'판시 사항 : {court_Decision[index[0][i]]}')\n",
    "        if OX == 'O' :\n",
    "            print(f'유사한 판례 : {court_detail[index[0][i]]}')\n",
    "elif num > 5:\n",
    "    OX = str(input('판례 내용을 보시겠습니까?(O, X) : '))\n",
    "    print(f'질문 : \"{detail}\"')\n",
    "    index = model_with_pretrained_tfidf(detail, 5, court_detail)\n",
    "    for i in index[0][:5] :\n",
    "        print(f'사건 번호 : {court_num[index[0][i]]}')\n",
    "        print(f'참조 조문 : {court_reference[index[0][i]]}')\n",
    "        print(f'판시 사항 : {court_Decision[index[0][i]]}')\n",
    "        if OX == 'O' :\n",
    "            print(f'유사한 판례 : {court_detail[index[0][i]]}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb1bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fcb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b009d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12246d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1a153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fe292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15512f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.request import urlopen\n",
    "from tqdm import trange\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import re\n",
    "\n",
    "my_id = 'realstone.in'\n",
    "org = '400201' # 대법원\n",
    "JO = '근로' #%EB%AF%BC%EB%B2%95\n",
    "base_url = f'https://www.law.go.kr/DRF/lawSearch.do?OC={my_id}&target=prec&type=XML&display=100'\n",
    "event_base_url = 'http://www.law.go.kr'\n",
    "\n",
    "'''\n",
    "https://www.law.go.kr/DRF/lawSearch.do?OC=realstone.in&target=prec&type=XML&display=100&page=1&search=3&org=400201&JO=%EB%AF%BC%EB%B2%95\n",
    "'''\n",
    "\n",
    "# 사건명 사건번호 사건종류명 판례일련번호 판례상세링크 법원명\n",
    "event_Title = []\n",
    "event_num = []\n",
    "event_type = []\n",
    "jPrecedent_num = []\n",
    "jPrecedent_link_get = []\n",
    "jPrecedent_link_repl = []\n",
    "court_name = []\n",
    "court_num = []\n",
    "court_detail = []\n",
    "court_reference = []\n",
    "court_Decision = []\n",
    "page=1\n",
    "url = f'{base_url}&page={page}&JO={JO}'\n",
    "#&org={org}\n",
    "response = requests.get(url)\n",
    "title_XML = response.text\n",
    "title_soup = BeautifulSoup(title_XML, 'xml') \n",
    "\n",
    "total_page = int(title_soup.select('totalCnt')[0].string)//100+1\n",
    "\n",
    "분류 코드 \n",
    "for page in tqdm(range(1, total_page+1)) :\n",
    "    url = f'{base_url}&page={page}&org={org}&JO={JO}'\n",
    "    response = requests.get(url)\n",
    "    title_XML = response.text\n",
    "    title_soup = BeautifulSoup(title_XML, 'xml') \n",
    "\n",
    "    for i in range(len(title_soup.select('사건명'))) :\n",
    "        event_Title.append(title_soup.select('사건명')[i].string)\n",
    "        event_num.append(title_soup.select('사건번호')[i].string)\n",
    "        event_type.append(title_soup.select('사건종류명')[i].string)\n",
    "        jPrecedent_num.append(title_soup.select('판례일련번호')[i].string)\n",
    "        jPrecedent_link_get.append(title_soup.select('판례상세링크')[i].string)\n",
    "        court_name.append(title_soup.select('법원명')[i].string)\n",
    "        \n",
    "\n",
    "for i in tqdm(range(len(jPrecedent_link_get))) :\n",
    "    jPrecedent_link_repl.append(jPrecedent_link_get[i][:-10].replace('HTML', 'XML'))\n",
    "    event_url = f'{event_base_url}{jPrecedent_link_repl[i]}'\n",
    "    event_response = requests.get(event_url)\n",
    "    main_XMLevent = event_response.text\n",
    "    main_soup = BeautifulSoup(main_XMLevent, 'xml')\n",
    "    #print(event_url)\n",
    "    court_num.append(main_soup.select('사건번호')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_detail.append(main_soup.select('판례내용')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_reference.append(main_soup.select('참조조문')[0].string.replace('</br>', '').replace('<br/>', ''))\n",
    "    court_Decision.append(main_soup.select('판시사항')[0].string.replace('</br>', '').replace('<br/>', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62650a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "'''\n",
    "event_Title \n",
    "event_num \n",
    "event_type \n",
    "jPrecedent_num \n",
    "jPrecedent_link_get \n",
    "court_name \n",
    "court_num \n",
    "court_detail \n",
    "court_reference \n",
    "court_Decision\n",
    "'''\n",
    "\n",
    "df = pd.DataFrame({'event_Title':event_Title, 'event_num':event_num, 'event_type':event_type, 'jPrecedent_num':jPrecedent_num, \\\n",
    "                  'jPrecedent_link_get':jPrecedent_link_get, 'court_name':court_name, 'court_num':court_num, \\\n",
    "                   'court_detail':court_detail, 'court_reference':court_reference, 'court_Decision':court_Decision})\n",
    "\n",
    "df.to_csv('court_work.csv',encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fa3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('court_work.csv', encoding='cp949')\n",
    "# 줄거리 텍스트 데이터 전처리\n",
    "okt = Okt()\n",
    "text = df['court_detail'].str.replace(\"\\n\", '')\n",
    "\n",
    "# 텍스트를 형태소 단위로 나누기\n",
    "processed_text = [' '.join(okt.morphs(sentence)) for sentence in text]\n",
    "\n",
    "# TF-IDF 벡터라이저 초기화 및 학습\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_text)\n",
    "\n",
    "# 모델과 TF-IDF 매트릭스 저장\n",
    "with open('court_tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "with open('court_tfidf_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix, f)\n",
    "\n",
    "# 전처리된 텍스트 저장\n",
    "with open('court_processed_text.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf17f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# 영화 데이터 로드\n",
    "df = pd.read_csv('court_work.csv', encoding='cp949')\n",
    "\n",
    "# 저장된 모델과 매트릭스 불러오기\n",
    "with open('court_tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "with open('court_tfidf_matrix.pkl', 'rb') as f:\n",
    "    tfidf_matrix = pickle.load(f)\n",
    "\n",
    "with open('court_processed_text.pkl', 'rb') as f:\n",
    "    processed_text = pickle.load(f)\n",
    "\n",
    "def recommend_court(input_sentence, top_n=5):\n",
    "    # 입력 문장을 Okt를 사용하여 전처리\n",
    "    okt = Okt()\n",
    "    processed_input = ' '.join(okt.morphs(input_sentence))\n",
    "    \n",
    "    # 입력 문장을 TF-IDF 변환\n",
    "    input_tfidf = tfidf_vectorizer.transform([processed_input])\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    cosine_similarities = cosine_similarity(input_tfidf, tfidf_matrix).flatten()\n",
    "    \n",
    "    # 유사도 순으로 정렬된 상위 n개 인덱스 추출\n",
    "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # 유사도 0인 경우 처리\n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        if cosine_similarities[idx] == 0:\n",
    "            break\n",
    "        recommendations.append((df.iloc[idx]['event_Title'], \\\n",
    "                                df.iloc[idx]['event_num'], \\\n",
    "                                df.iloc[idx]['event_type'], \\\n",
    "                                df.iloc[idx]['jPrecedent_num'], \\\n",
    "                                df.iloc[idx]['court_name'], \\\n",
    "                                df.iloc[idx]['court_num'], \\\n",
    "                                df.iloc[idx]['court_detail'], \\\n",
    "                                df.iloc[idx]['court_reference'], \\\n",
    "                                df.iloc[idx]['court_Decision'], \\\n",
    "                                cosine_similarities[idx]))\n",
    "    \n",
    "    if len(recommendations) == 0:\n",
    "        return \"유사한 판례를 찾을 수 없습니다.\"\n",
    "    else:\n",
    "        return recommendations\n",
    "\n",
    "'''\n",
    "event_Title \n",
    "event_num \n",
    "event_type \n",
    "jPrecedent_num \n",
    "court_name \n",
    "court_num \n",
    "court_detail \n",
    "court_reference \n",
    "court_Decision\n",
    "'''\n",
    "\n",
    "print('상황을 작성해 주세요 (자세히 작성 시 검색에 도움이 됩니다.) :')\n",
    "court_input = str(input())\n",
    "print('판례 및 판시 사항을 보시겠습니까?(Y/N) : ')\n",
    "court_detail_OX = str(input())\n",
    "input_sentence = court_input\n",
    "\n",
    "# 판례\n",
    "recommended_court = recommend_court(input_sentence)\n",
    "if isinstance(recommended_court, str):\n",
    "    print(recommended_court)\n",
    "else:\n",
    "    for event_Title, event_num, event_type, jPrecedent_num, court_name, court_num, court_detail, court_reference, court_Decision, similarity  in recommended_court:\n",
    "        if (court_detail_OX == 'Y') or (court_detail_OX == 'y') :\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f})\\n사건 번호 : {event_num} \\n사건 종류 : {event_type} \\n판례 일련번호 : {jPrecedent_num} \\\n",
    "            \\n법원 : {court_name} \\n 사건 번호 : {court_num} \\n\\n 판례 내용 :\\n{court_detail} \\n\\n 참조조문 \\n{court_reference} \\\n",
    "            \\n\\n판시사항 :\\n{court_Decision}\\n\\n\\n\")\n",
    "        elif (court_detail_OX == 'N') or (court_detail_OX == 'n') :\n",
    "            print(f\"사건 명 : {event_Title} (유사도: {similarity:.2f})\\n사건 번호 : {event_num} \\n사건 종류 : {event_type} \\n판례 일련번호 : {jPrecedent_num} \\\n",
    "            \\n법원 : {court_name} \\n 사건 번호 : {court_num} \\n\\n 참조조문 \\n{court_reference}\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "court_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461ab90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7fbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
